{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9HYDoBfH6hG",
        "outputId": "b0b0df28-201c-450b-a01f-9aa15ed20ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GAp9FxGOfvUC",
        "outputId": "07975fa7-5103-4880-a3c1-49ae6f343ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets transformers\n",
        "!pip install datasets transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "i84fGxmJH84y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/My Drive/nlp_hw4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKXNtj0WIHBT",
        "outputId": "05c21a0f-9ce4-4573-c523-25371b4f44c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.decomposition import PCA\n",
        "from gensim.models import Word2Vec\n",
        "import nltk\n",
        "from nltk import download\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "nltk.download('punkt')\n",
        "download('stopwords')\n",
        "download('wordnet')\n",
        "download('omw-1.4')\n",
        "random.seed(42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rcrLSA7IKd8",
        "outputId": "c6aec927-14b1-4d6c-d08b-49a79e6d6f0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "s8MrUtx9Ig4V"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HrCRYozpKgZB"
      },
      "source": [
        "# read data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "na7KP-aAKh2R"
      },
      "outputs": [],
      "source": [
        "train_data_np = np.load('/content/drive/My Drive/IMDB/x_train.npy', allow_pickle=True)\n",
        "val_data_np = np.load('/content/drive/My Drive/IMDB/x_val.npy', allow_pickle=True)\n",
        "test_data_np = np.load('/content/drive/My Drive/IMDB/x_test.npy', allow_pickle=True)\n",
        "unsupervised_data_np = np.load('/content/drive/My Drive/IMDB/unsupervised.npy', allow_pickle=True)\n",
        "\n",
        "df_train = pd.DataFrame(train_data_np)\n",
        "df_val = pd.DataFrame(val_data_np)\n",
        "df_test = pd.DataFrame(test_data_np, columns=[\"sentence\"])\n",
        "\n",
        "labels_train = pd.read_csv('/content/drive/My Drive/IMDB/y_train.txt', header=None)\n",
        "labels_val = pd.read_csv('/content/drive/My Drive/IMDB/y_val.txt', header=None)\n",
        "\n",
        "df_train_combined = pd.concat([df_train, labels_train], axis=1)\n",
        "df_train_combined.columns = [\"sentence\", \"label\"]\n",
        "df_train_combined['label'] = df_train_combined['label'].astype(int)\n",
        "\n",
        "df_val_combined = pd.concat([df_val, labels_val], axis=1)\n",
        "df_val_combined.columns = [\"sentence\", \"label\"]\n",
        "df_val_combined['label'] = df_val_combined['label'].astype(int)\n",
        "\n",
        "df_unsupervised = pd.DataFrame(unsupervised_data_np, columns=[\"sentence\"])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xAwW5xPWLaG7"
      },
      "outputs": [],
      "source": [
        "def split_into_sentences(text):\n",
        "    return sent_tokenize(text)\n",
        "\n",
        "split_sentences = []\n",
        "for text in df_unsupervised['sentence']:\n",
        "    split_sentences.extend(split_into_sentences(text))\n",
        "\n",
        "df_split_sentences = pd.DataFrame(split_sentences, columns=[\"sentence\"])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cMFNv32JLfQi"
      },
      "source": [
        "# 1 preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jMfeJi3kLhOl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def preprocess_text_data_fast(df, text_column):\n",
        "\n",
        "    stops = set(stopwords.words('english'))\n",
        "    processed_texts = []\n",
        "\n",
        "    for text in df[text_column]:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'<[^>]+>', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        words = text.split()\n",
        "        filtered_words = [word for word in words if word not in stops]\n",
        "        cleaned_text = ' '.join(filtered_words)\n",
        "        processed_texts.append(cleaned_text)\n",
        "\n",
        "    # Create a new column in the DataFrame for the processed text\n",
        "    df[text_column + '_clean'] = processed_texts\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YdK9ytDhMCwX"
      },
      "outputs": [],
      "source": [
        "df_sentences_processed = preprocess_text_data_fast(df_split_sentences, 'sentence')\n",
        "df_train_processed = preprocess_text_data_fast(df_train_combined, 'sentence')\n",
        "df_val_processed = preprocess_text_data_fast(df_val_combined, 'sentence')\n",
        "df_test_processed = preprocess_text_data_fast(df_test, 'sentence')\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vG9KF-aBMS3S"
      },
      "source": [
        "# 2 BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GqBpBXWLMUMU"
      },
      "outputs": [],
      "source": [
        "def save_text_to_file(dataframe, text_column, file_path):\n",
        "    \"\"\"\n",
        "    Save text data from a DataFrame column to a text file.\n",
        "\n",
        "    Args:\n",
        "    dataframe (pd.DataFrame): The DataFrame containing the text data.\n",
        "    text_column (str): The name of the column containing the text data.\n",
        "    file_path (str): The file path where the text data will be saved.\n",
        "    \"\"\"\n",
        "    text_data = dataframe[text_column].tolist()\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        file.writelines(f\"{line}\\n\" for line in text_data)\n",
        "\n",
        "output_file_path = '/content/drive/My Drive/nlp_hw4/unsupervised_data.txt'\n",
        "save_text_to_file(df_sentences_processed, 'sentence_clean', output_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PADj4FbRQTBt"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# initialize a Byte-Pair Encoding (BPE) model tokenizer\n",
        "bpe_tokenizer = Tokenizer(BPE())\n",
        "\n",
        "# Define a BPE trainer with specific special tokens and vocab size\n",
        "vocab_size = 30000  # Set the desired vocabulary size\n",
        "bpe_trainer = BpeTrainer(special_tokens=[\"<UNK>\", \"<CLS>\", \"<SEP>\", \"<PAD>\", \"<MASK>\"], vocab_size=vocab_size)\n",
        "\n",
        "# set the tokenizer to use whitespace pre-tokenization\n",
        "bpe_tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "# train the tokenizer on the preprocessed unsupervised data file\n",
        "unsupervised_data_file = \"/content/drive/My Drive/nlp_hw4/unsupervised_data.txt\"\n",
        "bpe_tokenizer.train(files=[unsupervised_data_file], trainer=bpe_trainer)\n",
        "\n",
        "tokenizer_path = \"/content/drive/My Drive/nlp_hw4/tokenizer.json\"\n",
        "bpe_tokenizer.save(tokenizer_path)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1lDb8PLGIz9X"
      },
      "source": [
        "# 3 encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3x6iZDA_Ql1G"
      },
      "outputs": [],
      "source": [
        "def encode_text_column(df, text_column, tokenizer):\n",
        "\n",
        "    tokens_list = []\n",
        "    ids_list = []\n",
        "\n",
        "    for text in df[text_column]:\n",
        "        encoded = tokenizer.encode(text)\n",
        "        tokens_list.append(encoded.tokens)\n",
        "        ids_list.append(encoded.ids)\n",
        "\n",
        "    df[text_column + \"_tokens\"] = tokens_list\n",
        "    df[text_column + \"_ids\"] = ids_list\n",
        "    return df\n",
        "\n",
        "df_train_encoded = encode_text_column(df_train_processed, 'sentence_clean', bpe_tokenizer)\n",
        "df_val_encoded = encode_text_column(df_val_processed, 'sentence_clean', bpe_tokenizer)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_3GCP0hQzrg",
        "outputId": "60a4d182-1bce-47f3-ddfe-12b75273a2f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0     39\n",
            "1    356\n",
            "2     87\n",
            "3     54\n",
            "4    252\n",
            "Name: sentence_clean_tokens, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def calculate_token_lengths(df, tokens_column):\n",
        "\n",
        "    return df[tokens_column].apply(len)\n",
        "\n",
        "token_lengths_train = calculate_token_lengths(df_train_encoded, 'sentence_clean_tokens')\n",
        "\n",
        "print(token_lengths_train.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5NfrLZlQ3xn",
        "outputId": "79736ba7-68f5-455d-e8a9-c467267cfd4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[ 1065,  1065,  1233,  ...,     0,     0,     0],\n",
            "        [ 1173,   537,   182,  ...,     0,     0,     0],\n",
            "        [ 2462,  1866,  3967,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [16770,  3465, 16770,  ..., 18618, 10388,  3411],\n",
            "        [  361,  1725,   888,  ...,   878,   349,  1802],\n",
            "        [  568,   674,  2403,  ...,     0,     0,     0]]), 'label': tensor([1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1,\n",
            "        1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
            "        1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0])}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataSequence(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, max_seq_len=150):\n",
        "        self.dataframe = dataframe\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.dataframe.iloc[index]['label']\n",
        "        token_ids = self.dataframe.iloc[index]['sentence_clean_ids']\n",
        "\n",
        "        # ensure the sequence is of maximum length\n",
        "        if len(token_ids) > self.max_seq_len:\n",
        "            token_ids = token_ids[:self.max_seq_len]\n",
        "        else:\n",
        "            token_ids += [0] * (self.max_seq_len - len(token_ids))\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = TextDataSequence(df_train_encoded)\n",
        "validation_dataset = TextDataSequence(df_val_encoded)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for batch in train_loader:\n",
        "    print(batch)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5OfbyC7LRZOn"
      },
      "outputs": [],
      "source": [
        "\n",
        "embedding_dimension = 128\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#learning_rate = 1e-6\n",
        "#hidden_layer_size = 512\n",
        "num_classes = 2\n",
        "vocab_size = bpe_tokenizer.get_vocab_size()\n",
        "num_attention_heads = 8\n",
        "feedforward_dim = 1028\n",
        "transformer_layers = 2\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYuhj6gBRjXW",
        "outputId": "eb04cf19-04c7-4b99-a9b5-84a8f79423f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 2])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TextTransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, num_classes, n_heads, ff_hidden_dim, n_layers, dropout_rate):\n",
        "        super(TextTransformerModel, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, 512, embedding_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=n_heads, dim_feedforward=ff_hidden_dim, dropout=dropout_rate)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
        "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        # token and positional embeddings\n",
        "        seq_len = input_ids.size(1)\n",
        "        embeddings = self.token_embedding(input_ids) + self.positional_encoding[:, :seq_len, :]\n",
        "\n",
        "        # transformer encoding\n",
        "        transformer_output = self.transformer_encoder(embeddings)\n",
        "\n",
        "        # global average pooling\n",
        "        pooled_output = transformer_output.mean(dim=1)\n",
        "\n",
        "        # classification layer\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "model = TextTransformerModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dimension,\n",
        "    num_classes=num_classes,\n",
        "    n_heads=num_attention_heads,\n",
        "    ff_hidden_dim=feedforward_dim,\n",
        "    n_layers=transformer_layers,\n",
        "    dropout_rate = dropout_rate\n",
        ")\n",
        "\n",
        "example_input = torch.randint(0, vocab_size, (32, 50))\n",
        "\n",
        "# forward pass\n",
        "output_logits = model(example_input)\n",
        "print(output_logits.shape)  # expected output shape: (32, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6jn8uPefSfwu"
      },
      "outputs": [],
      "source": [
        "def compute_accuracy(predictions, targets):\n",
        "\n",
        "    correct_preds = (predictions.argmax(dim=1) == targets).sum().item()\n",
        "    accuracy = correct_preds / len(targets)\n",
        "    return accuracy\n",
        "\n",
        "def train_model(model, dataloader, optimizer, loss_function):\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    model.train()\n",
        "\n",
        "    progress_bar = tqdm(total=len(dataloader), desc=\"Training\", unit=\"batch\")\n",
        "\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(inputs).squeeze(1)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = compute_accuracy(outputs.cpu(), labels.cpu())\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    average_accuracy = total_accuracy / len(dataloader)\n",
        "\n",
        "    return average_loss, average_accuracy\n",
        "\n",
        "def evaluate_model(model, dataloader, loss_function):\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    model.eval()\n",
        "\n",
        "    progress_bar = tqdm(total=len(dataloader), desc=\"Evaluating\", unit=\"batch\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(inputs).squeeze(1)\n",
        "            loss = loss_function(outputs, labels)\n",
        "\n",
        "            accuracy = compute_accuracy(outputs.cpu(), labels.cpu())\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += accuracy\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    average_accuracy = total_accuracy / len(dataloader)\n",
        "\n",
        "    return average_loss, average_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSL4OvioSnoa",
        "outputId": "3f0883f5-9da8-4716-a962-60b3d31a17e1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 50.62batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Loss: 0.70 | Validation Accuracy: 50.03%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model.to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "validation_loss, validation_accuracy = evaluate_model(model, validation_loader, loss_function)\n",
        "print(f'\\nValidation Loss: {validation_loss:.2f} | Validation Accuracy: {validation_accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ptJKujWVSyXA",
        "outputId": "9fc007ad-d07c-4d55-bcf3-546600444688"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 28.71batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 60.90batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.57 | Train Accuracy: 69.01%\n",
            "Validation Loss: 0.48 | Validation Accuracy: 77.78%\n",
            "Epoch Time: 16.92 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 28.37batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 61.08batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n",
            "Train Loss: 0.41 | Train Accuracy: 81.67%\n",
            "Validation Loss: 0.43 | Validation Accuracy: 81.15%\n",
            "Epoch Time: 17.07 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 28.44batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 54.10batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10\n",
            "Train Loss: 0.34 | Train Accuracy: 85.59%\n",
            "Validation Loss: 0.43 | Validation Accuracy: 81.66%\n",
            "Epoch Time: 17.39 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 28.54batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 49.61batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10\n",
            "Train Loss: 0.29 | Train Accuracy: 87.93%\n",
            "Validation Loss: 0.43 | Validation Accuracy: 82.04%\n",
            "Epoch Time: 17.67 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 28.60batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 60.15batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10\n",
            "Train Loss: 0.25 | Train Accuracy: 90.09%\n",
            "Validation Loss: 0.43 | Validation Accuracy: 83.02%\n",
            "Epoch Time: 16.94 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 28.72batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 60.41batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10\n",
            "Train Loss: 0.20 | Train Accuracy: 91.84%\n",
            "Validation Loss: 0.44 | Validation Accuracy: 83.01%\n",
            "Epoch Time: 16.88 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 28.54batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:04<00:00, 47.49batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10\n",
            "Train Loss: 0.18 | Train Accuracy: 93.13%\n",
            "Validation Loss: 0.48 | Validation Accuracy: 83.05%\n",
            "Epoch Time: 17.84 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 28.83batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 55.40batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10\n",
            "Train Loss: 0.14 | Train Accuracy: 94.65%\n",
            "Validation Loss: 0.54 | Validation Accuracy: 82.99%\n",
            "Epoch Time: 17.11 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 28.64batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 60.88batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10\n",
            "Train Loss: 0.12 | Train Accuracy: 95.53%\n",
            "Validation Loss: 0.55 | Validation Accuracy: 83.44%\n",
            "Epoch Time: 16.89 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:13<00:00, 28.83batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 60.81batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10\n",
            "Train Loss: 0.10 | Train Accuracy: 96.43%\n",
            "Validation Loss: 0.63 | Validation Accuracy: 83.31%\n",
            "Epoch Time: 16.80 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# initialize the best validation loss to infinity\n",
        "lowest_val_loss = float('inf')\n",
        "\n",
        "# training loop\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    training_loss, training_accuracy = train_model(model, train_loader, optimizer, loss_function)\n",
        "\n",
        "    validation_loss, validation_accuracy = evaluate_model(model, validation_loader, loss_function)\n",
        "\n",
        "    # save the model if the validation loss is the best we've seen so far\n",
        "    if validation_loss < lowest_val_loss:\n",
        "        torch.save(model.state_dict(), '/content/drive/My Drive/nlp_hw4/best_model.pt')\n",
        "        lowest_val_loss = validation_loss\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print(f'Train Loss: {training_loss:.2f} | Train Accuracy: {training_accuracy * 100:.2f}%')\n",
        "    print(f'Validation Loss: {validation_loss:.2f} | Validation Accuracy: {validation_accuracy * 100:.2f}%')\n",
        "    print(f'Epoch Time: {time.time() - start_time:.2f} seconds')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WD9peePOTzeY"
      },
      "source": [
        "# 4 predict ofr test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDdYSYugUNKZ",
        "outputId": "f32aa5ca-e7a7-48c3-9af2-db3b5546fd69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "                                            sentence  \\\n",
            "0  <br /><br />I'm not sure who decides what cate...   \n",
            "\n",
            "                                      sentence_clean  \\\n",
            "0  im sure decides category movie fits movie horr...   \n",
            "\n",
            "                               sentence_clean_tokens  \\\n",
            "0  [im, sure, decides, category, movie, fits, mov...   \n",
            "\n",
            "                                  sentence_clean_ids  \n",
            "0  [176, 693, 2650, 5193, 182, 3759, 182, 636, 18...  \n"
          ]
        }
      ],
      "source": [
        "print(type(df_test_processed))\n",
        "print(df_test_processed[:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "uT9sdgVAT3CP"
      },
      "outputs": [],
      "source": [
        "# Modify TextDataSequence to handle test data without labels\n",
        "class TextDataSequence(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch Dataset for text data sequences.\n",
        "\n",
        "    Args:\n",
        "    dataframe (pd.DataFrame): DataFrame containing the text data and optionally labels.\n",
        "    max_seq_len (int): Maximum length of token sequences.\n",
        "    has_labels (bool): Whether the dataframe contains labels.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataframe, max_seq_len=150, has_labels=True):\n",
        "        self.dataframe = dataframe\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.has_labels = has_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        token_ids = self.dataframe.iloc[index]['sentence_clean_ids']\n",
        "\n",
        "        # ensure the sequence is of maximum length\n",
        "        if len(token_ids) > self.max_seq_len:\n",
        "            token_ids = token_ids[:self.max_seq_len]\n",
        "        else:\n",
        "            token_ids += [0] * (self.max_seq_len - len(token_ids))\n",
        "\n",
        "        item = {\n",
        "            'input_ids': torch.tensor(token_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "        if self.has_labels:\n",
        "            label = self.dataframe.iloc[index]['label']\n",
        "            item['label'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return item\n",
        "\n",
        "# encode the test data using the BPE tokenizer\n",
        "df_test_encoded = encode_text_column(df_test_processed, 'sentence_clean', bpe_tokenizer)\n",
        "\n",
        "test_dataset = TextDataSequence(df_test_encoded, has_labels=False)\n",
        "\n",
        "batch_size = 64  \n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "def predict_model(model, dataloader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "    return all_predictions\n",
        "\n",
        "test_predictions = predict_model(model, test_loader)\n",
        "\n",
        "with open('/content/drive/My Drive/nlp_hw4/y_test.txt', 'w') as file:\n",
        "    for prediction in test_predictions:\n",
        "        file.write(f\"{float(prediction)}\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y-t1LoXTJLc"
      },
      "source": [
        "# 5 training bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 740,
          "referenced_widgets": [
            "81bec07a5fac492b92e1b30d986bd0fb",
            "855c69c0477d44108aa96cb4b7a78b22",
            "64d2aa10b75442749def81be47867277",
            "8edd59f8bc7a4789980de5c312d447cc",
            "cb53914a9e1d40c29b6aab2c6e68fb2e",
            "d145cc2396e247ca918f284ae011e0fe",
            "11437c7066284bdcaec01a5a4b4da35c",
            "dd5b4ab2b79d461486b99c58f85f5e6d",
            "9bb168712c1846438dc29e8c6b85dbf1",
            "cf89d3402ea445fca94344c562c4cb68",
            "7d37b3bf65404467afaaeabdf28e0a8a",
            "5ab4213a3357430088738141634ad504",
            "0351806b30c143e098b4ad22aaf8a80a",
            "748e4c7b07f44917ae23de759345ebd1",
            "d5c3eeb2cb9c426f9688943a3a7504e5",
            "c9e11ec23fe049b58a35cf194126d0da",
            "d646c957bc4c4ec1a9b4c891895f67c8",
            "b2fe59760f2d4c72b47c73a012b3e602",
            "0325e4ce4a9942469cf75e087980e669",
            "097d46637ea54c9b89467a89f9003f52",
            "584b7f69251a469b9aa564e2ce25520c",
            "2ffcfc6ff0cb4a8caaa39390fa1ef108"
          ]
        },
        "id": "CK42FfgnA24k",
        "outputId": "2f8969af-7c44-4ec1-e85d-e855539f120b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81bec07a5fac492b92e1b30d986bd0fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ab4213a3357430088738141634ad504",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7820' max='7820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7820/7820 12:52, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>9.141300</td>\n",
              "      <td>8.758295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>8.513800</td>\n",
              "      <td>8.501783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>8.493400</td>\n",
              "      <td>8.467978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>8.464800</td>\n",
              "      <td>8.446367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>8.461100</td>\n",
              "      <td>8.454931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>8.448000</td>\n",
              "      <td>8.449119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>8.446200</td>\n",
              "      <td>8.439943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>8.437300</td>\n",
              "      <td>8.443792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>8.447000</td>\n",
              "      <td>8.435773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>8.445300</td>\n",
              "      <td>8.431726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>8.419300</td>\n",
              "      <td>8.423152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>8.419200</td>\n",
              "      <td>8.426134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>8.418300</td>\n",
              "      <td>8.415391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>8.424500</td>\n",
              "      <td>8.417845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>8.417600</td>\n",
              "      <td>8.428065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 4,696,752 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertConfig, BertForMaskedLM, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "# load the BPE tokenizer you trained and saved\n",
        "tokenizer_path = \"/content/drive/My Drive/nlp_hw4/tokenizer.json\"\n",
        "bpe_tokenizer = Tokenizer.from_file(tokenizer_path)\n",
        "\n",
        "# define BERT configuration with specified parameters\n",
        "bert_config = BertConfig(\n",
        "    vocab_size=bpe_tokenizer.get_vocab_size(),  # This should be the same as tokenizer's vocab size\n",
        "    hidden_size=128,  # embedding size\n",
        "    num_hidden_layers=4,  # number of transformer layers\n",
        "    num_attention_heads=4,  # number of attention heads\n",
        "    intermediate_size=512,  # size of the feedforward layer\n",
        "    max_position_embeddings=128  # this should match the max sequence length\n",
        ")\n",
        "\n",
        "# initialize the BERT model for Masked Language Modeling\n",
        "bert_model = BertForMaskedLM(config=bert_config)\n",
        "\n",
        "# tokenize the text data using the BPE tokenizer\n",
        "def tokenize_function(examples):\n",
        "    return {'input_ids': [bpe_tokenizer.encode(text).ids for text in examples['sentence_clean']]}\n",
        "\n",
        "# convert DataFrames to Hugging Face datasets\n",
        "train_dataset = Dataset.from_pandas(df_train_processed[['sentence_clean']])\n",
        "val_dataset = Dataset.from_pandas(df_val_processed[['sentence_clean']])\n",
        "\n",
        "# apply the tokenization function to the datasets\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['sentence_clean'])\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['sentence_clean'])\n",
        "\n",
        "# define a custom data collator for MLM\n",
        "class CustomDataCollator:\n",
        "    def __init__(self, tokenizer, mlm=True, mlm_probability=0.15):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mlm = mlm\n",
        "        self.mlm_probability = mlm_probability\n",
        "        self.pad_token_id = tokenizer.token_to_id('<PAD>')\n",
        "        self.mask_token_id = tokenizer.token_to_id('<MASK>')\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        input_ids = [ex['input_ids'] for ex in examples]\n",
        "        input_ids = self.pad_sequences(input_ids, 128)\n",
        "        inputs, labels = self.mask_tokens(torch.tensor(input_ids, dtype=torch.long))\n",
        "        return {'input_ids': inputs, 'labels': labels}\n",
        "\n",
        "    def pad_sequences(self, sequences, max_length):\n",
        "        padded_sequences = []\n",
        "        for seq in sequences:\n",
        "            if len(seq) > max_length:\n",
        "                seq = seq[:max_length]\n",
        "            else:\n",
        "                seq = seq + [self.pad_token_id] * (max_length - len(seq))\n",
        "            padded_sequences.append(seq)\n",
        "        return padded_sequences\n",
        "\n",
        "    def mask_tokens(self, inputs):\n",
        "        labels = inputs.clone()\n",
        "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
        "        special_tokens_mask = self.get_special_tokens_mask(labels.tolist())\n",
        "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "        labels[~masked_indices] = -100  # we only compute loss on masked tokens\n",
        "        inputs[masked_indices] = self.mask_token_id\n",
        "        return inputs, labels\n",
        "\n",
        "    def get_special_tokens_mask(self, input_ids):\n",
        "        # manually create the special tokens mask\n",
        "        special_tokens_mask = []\n",
        "        for ids in input_ids:\n",
        "            mask = [1 if token in [self.pad_token_id, self.mask_token_id] else 0 for token in ids]\n",
        "            special_tokens_mask.append(mask)\n",
        "        return special_tokens_mask\n",
        "\n",
        "data_collator = CustomDataCollator(tokenizer=bpe_tokenizer, mlm=True, mlm_probability=0.15)\n",
        "\n",
        "#  training arguments:\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=200,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=bert_model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "model_save_path = '/content/drive/My Drive/nlp_hw4/bert-small-mlm'\n",
        "bert_model.save_pretrained(model_save_path)\n",
        "bpe_tokenizer.save(model_save_path + '/bpe_tokenizer.json')\n",
        "\n",
        "#function to count the number of parameters in the model\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "num_params = count_parameters(bert_model)\n",
        "print(f'The model has {num_params:,} trainable parameters')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UiM0qkZRdPhK"
      },
      "outputs": [],
      "source": [
        "model_save_path = '/content/drive/My Drive/nlp_hw4/bert-small-mlm'\n",
        "bert_model.save_pretrained(model_save_path)\n",
        "bpe_tokenizer.save(model_save_path + '/bpe_tokenizer.json')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz3MAq9SexbO"
      },
      "source": [
        "# measurment score for custom bert:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hwlWX2adRYh",
        "outputId": "1bbf3e1a-e150-4d4c-d04f-d23a02e63041"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 196/196 [00:08<00:00, 22.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_save_path = '/content/drive/My Drive/nlp_hw4/bert-small-mlm'\n",
        "bert_model = BertForMaskedLM.from_pretrained(model_save_path)\n",
        "bpe_tokenizer = Tokenizer.from_file(model_save_path + '/bpe_tokenizer.json')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "bert_model.to(device)\n",
        "\n",
        "class TextDatasetWithAttention(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, max_seq_len=128, pad_token_id=0):\n",
        "        self.dataframe = dataframe\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.dataframe.iloc[index]['label']\n",
        "        token_ids = self.dataframe.iloc[index]['sentence_clean_ids']\n",
        "\n",
        "        # ensure the sequence is of maximum length\n",
        "        if len(token_ids) > self.max_seq_len:\n",
        "            token_ids = token_ids[:self.max_seq_len]\n",
        "        else:\n",
        "            token_ids += [self.pad_token_id] * (self.max_seq_len - len(token_ids))\n",
        "\n",
        "        attention_mask = [1 if token_id != self.pad_token_id else 0 for token_id in token_ids]\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# use the pad_token_id from the tokenizer\n",
        "pad_token_id = bpe_tokenizer.token_to_id(\"<PAD>\") if \"<PAD>\" in bpe_tokenizer.get_vocab() else 0\n",
        "\n",
        "validation_dataset = TextDatasetWithAttention(df_val_encoded, pad_token_id=pad_token_id)\n",
        "\n",
        "batch_size = 64\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def compute_accuracy(predictions, labels):\n",
        "    _, preds = torch.max(predictions, dim=1)\n",
        "    correct = (preds == labels).sum().item()\n",
        "    accuracy = correct / len(labels)\n",
        "    return accuracy\n",
        "\n",
        "def evaluate_accuracy(model, dataloader):\n",
        "    model.eval()\n",
        "    total_accuracy = 0\n",
        "    total_examples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            outputs = model(input_ids=inputs, attention_mask=attention_mask).logits\n",
        "\n",
        "            predictions = outputs.argmax(dim=1)\n",
        "\n",
        "            accuracy = compute_accuracy(predictions, labels)\n",
        "            total_accuracy += accuracy * len(labels)\n",
        "            total_examples += len(labels)\n",
        "\n",
        "    avg_accuracy = total_accuracy / total_examples\n",
        "    return avg_accuracy\n",
        "\n",
        "validation_accuracy = evaluate_accuracy(bert_model, validation_loader)\n",
        "print(f'Validation Accuracy: {validation_accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-CDgeiWkaZGQ"
      },
      "source": [
        "# 6 fully connected on BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDbDWnjLb8C4",
        "outputId": "df0b559a-652d-4077-880c-c9900e7b0b97"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertModel were not initialized from the model checkpoint at /content/drive/My Drive/nlp_hw4/bert-small-mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Training: 100%|██████████| 391/391 [00:15<00:00, 25.84batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:04<00:00, 41.84batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.72 | Train Accuracy: 51.18%\n",
            "Validation Loss: 0.69 | Validation Accuracy: 54.34%\n",
            "Epoch Time: 19.91 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:14<00:00, 26.10batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 54.48batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n",
            "Train Loss: 0.69 | Train Accuracy: 55.38%\n",
            "Validation Loss: 0.66 | Validation Accuracy: 60.33%\n",
            "Epoch Time: 18.67 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:15<00:00, 25.84batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 52.33batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10\n",
            "Train Loss: 0.62 | Train Accuracy: 64.98%\n",
            "Validation Loss: 0.55 | Validation Accuracy: 72.54%\n",
            "Epoch Time: 19.00 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:15<00:00, 25.63batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 51.67batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10\n",
            "Train Loss: 0.44 | Train Accuracy: 79.50%\n",
            "Validation Loss: 0.41 | Validation Accuracy: 82.24%\n",
            "Epoch Time: 19.15 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:14<00:00, 26.19batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 56.07batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10\n",
            "Train Loss: 0.31 | Train Accuracy: 87.30%\n",
            "Validation Loss: 0.38 | Validation Accuracy: 83.93%\n",
            "Epoch Time: 18.51 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:14<00:00, 26.12batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:04<00:00, 41.98batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10\n",
            "Train Loss: 0.24 | Train Accuracy: 90.14%\n",
            "Validation Loss: 0.38 | Validation Accuracy: 84.60%\n",
            "Epoch Time: 19.66 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:14<00:00, 26.38batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 55.93batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10\n",
            "Train Loss: 0.20 | Train Accuracy: 92.50%\n",
            "Validation Loss: 0.40 | Validation Accuracy: 84.90%\n",
            "Epoch Time: 18.34 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:14<00:00, 26.12batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 55.87batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10\n",
            "Train Loss: 0.16 | Train Accuracy: 93.95%\n",
            "Validation Loss: 0.43 | Validation Accuracy: 84.74%\n",
            "Epoch Time: 18.49 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:15<00:00, 25.83batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:04<00:00, 43.53batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10\n",
            "Train Loss: 0.13 | Train Accuracy: 95.03%\n",
            "Validation Loss: 0.48 | Validation Accuracy: 84.12%\n",
            "Epoch Time: 19.66 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 391/391 [00:15<00:00, 25.85batch/s]\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 56.55batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10\n",
            "Train Loss: 0.11 | Train Accuracy: 96.17%\n",
            "Validation Loss: 0.48 | Validation Accuracy: 85.30%\n",
            "Epoch Time: 18.61 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe, max_seq_len=128):\n",
        "        self.dataframe = dataframe\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        label = self.dataframe.iloc[index]['label']\n",
        "        token_ids = self.dataframe.iloc[index]['sentence_clean_ids']\n",
        "\n",
        "        # ensure the sequence is of maximum length\n",
        "        if len(token_ids) > self.max_seq_len:\n",
        "            token_ids = token_ids[:self.max_seq_len]\n",
        "        else:\n",
        "            token_ids += [0] * (self.max_seq_len - len(token_ids))\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "train_dataset = TextDataset(df_train_encoded)\n",
        "validation_dataset = TextDataset(df_val_encoded)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.last_hidden_state[:, 0, :]  # extract the [CLS] token output\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        logits = self.classifier(cls_output)\n",
        "        return logits\n",
        "\n",
        "# load the trained BERT model\n",
        "model_path = '/content/drive/My Drive/nlp_hw4/bert-small-mlm'\n",
        "bert_model = BertModel.from_pretrained(model_path)\n",
        "\n",
        "# freeze BERT model parameters\n",
        "for param in bert_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# instantiate the classifier with the BERT model\n",
        "num_classes = 2  # For binary classification\n",
        "classifier_model = BertClassifier(bert_model, num_classes)\n",
        "\n",
        "classifier_model.to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier_model.parameters(), lr=2e-5)\n",
        "\n",
        "def compute_accuracy(predictions, targets):\n",
        "    correct_preds = (predictions.argmax(dim=1) == targets).sum().item()\n",
        "    accuracy = correct_preds / len(targets)\n",
        "    return accuracy\n",
        "\n",
        "# Training function for the classifier\n",
        "def train_classifier(model, dataloader, optimizer, loss_function):\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    model.train()\n",
        "\n",
        "    progress_bar = tqdm(total=len(dataloader), desc=\"Training\", unit=\"batch\")\n",
        "\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        accuracy = compute_accuracy(outputs.cpu(), labels.cpu())\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    average_accuracy = total_accuracy / len(dataloader)\n",
        "\n",
        "    return average_loss, average_accuracy\n",
        "\n",
        "def evaluate_classifier(model, dataloader, loss_function):\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    model.eval()\n",
        "\n",
        "    progress_bar = tqdm(total=len(dataloader), desc=\"Evaluating\", unit=\"batch\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "\n",
        "            accuracy = compute_accuracy(outputs.cpu(), labels.cpu())\n",
        "            total_loss += loss.item()\n",
        "            total_accuracy += accuracy\n",
        "\n",
        "            progress_bar.update(1)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    average_accuracy = total_accuracy / len(dataloader)\n",
        "\n",
        "    return average_loss, average_accuracy\n",
        "\n",
        "epochs = 10\n",
        "lowest_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    training_loss, training_accuracy = train_classifier(classifier_model, train_loader, optimizer, loss_function)\n",
        "\n",
        "    validation_loss, validation_accuracy = evaluate_classifier(classifier_model, validation_loader, loss_function)\n",
        "\n",
        "    if validation_loss < lowest_val_loss:\n",
        "        torch.save(classifier_model.state_dict(), '/content/drive/My Drive/nlp_hw4/best_classifier_model.pt')\n",
        "        lowest_val_loss = validation_loss\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print(f'Train Loss: {training_loss:.2f} | Train Accuracy: {training_accuracy * 100:.2f}%')\n",
        "    print(f'Validation Loss: {validation_loss:.2f} | Validation Accuracy: {validation_accuracy * 100:.2f}%')\n",
        "    print(f'Epoch Time: {time.time() - start_time:.2f} seconds')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg7DykqcdQpX",
        "outputId": "ae62c320-974e-4e52-f9f1-4576bdd5ffb7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|██████████| 196/196 [00:02<00:00, 77.11it/s]\n"
          ]
        }
      ],
      "source": [
        "# Ensure test data is preprocessed and tokenized\n",
        "df_test_processed = preprocess_text_data_fast(df_test, 'sentence')\n",
        "df_test_encoded = encode_text_column(df_test_processed, 'sentence_clean', bpe_tokenizer)\n",
        "\n",
        "# Define the custom dataset for the test data\n",
        "class TextTestDataset(Dataset):\n",
        "    def __init__(self, dataframe, max_seq_len=128):\n",
        "        self.dataframe = dataframe\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        token_ids = self.dataframe.iloc[index]['sentence_clean_ids']\n",
        "\n",
        "        # Ensure the sequence is of maximum length\n",
        "        if len(token_ids) > self.max_seq_len:\n",
        "            token_ids = token_ids[:self.max_seq_len]\n",
        "        else:\n",
        "            token_ids += [0] * (self.max_seq_len - len(token_ids))\n",
        "\n",
        "        return {\n",
        "            'input_ids': torch.tensor(token_ids, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create Dataset instance\n",
        "test_dataset = TextTestDataset(df_test_encoded)\n",
        "\n",
        "# Create DataLoader instance\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load the trained classifier model\n",
        "classifier_model.load_state_dict(torch.load('/content/drive/My Drive/nlp_hw4/best_classifier_model.pt'))\n",
        "classifier_model.eval()\n",
        "\n",
        "# Prediction function\n",
        "def predict(model, dataloader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Predicting\"):\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "# Predict on the test data\n",
        "test_predictions = predict(classifier_model, test_loader)\n",
        "\n",
        "# Save predictions to a file\n",
        "output_file_path = '/content/drive/My Drive/nlp_hw4/y_test2.txt'\n",
        "np.savetxt(output_file_path, test_predictions, fmt='%d')\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mLvCpFL4dzss"
      },
      "source": [
        "# 7 train both bert and fully connected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKjN9Iycd31R",
        "outputId": "6d5e88fd-bd5c-4a20-bead-42f16b47a23c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/391 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   0%|          | 1/391 [00:00<01:00,  6.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 3/391 [00:00<00:32, 11.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|▏         | 5/391 [00:00<00:27, 14.26batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 7/391 [00:00<00:24, 15.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 10/391 [00:00<00:20, 19.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 13/391 [00:00<00:17, 21.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 16/391 [00:00<00:16, 22.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▍         | 19/391 [00:00<00:15, 23.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 22/391 [00:01<00:14, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▋         | 25/391 [00:01<00:14, 25.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 28/391 [00:01<00:14, 25.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 31/391 [00:01<00:14, 25.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▊         | 34/391 [00:01<00:14, 24.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 37/391 [00:01<00:14, 25.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|█         | 40/391 [00:01<00:13, 25.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 43/391 [00:01<00:13, 26.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 46/391 [00:01<00:12, 26.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 49/391 [00:02<00:12, 26.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 52/391 [00:02<00:12, 26.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 55/391 [00:02<00:12, 26.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▍        | 58/391 [00:02<00:12, 25.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 61/391 [00:02<00:12, 25.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▋        | 64/391 [00:02<00:12, 26.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 67/391 [00:02<00:12, 26.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 70/391 [00:02<00:12, 26.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▊        | 73/391 [00:03<00:12, 26.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 76/391 [00:03<00:11, 26.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|██        | 79/391 [00:03<00:11, 26.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 82/391 [00:03<00:12, 25.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 85/391 [00:03<00:11, 25.78batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 88/391 [00:03<00:11, 25.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 91/391 [00:03<00:11, 25.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 94/391 [00:03<00:11, 26.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▍       | 97/391 [00:03<00:11, 26.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 100/391 [00:04<00:10, 26.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▋       | 103/391 [00:04<00:10, 26.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 106/391 [00:04<00:10, 26.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 109/391 [00:04<00:10, 25.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▊       | 112/391 [00:04<00:10, 26.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 115/391 [00:04<00:10, 25.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|███       | 118/391 [00:04<00:10, 25.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 121/391 [00:04<00:10, 26.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 124/391 [00:04<00:10, 26.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 127/391 [00:05<00:09, 26.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 130/391 [00:05<00:09, 26.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 133/391 [00:05<00:09, 26.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▍      | 136/391 [00:05<00:09, 25.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 139/391 [00:05<00:09, 26.00batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▋      | 142/391 [00:05<00:09, 25.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 145/391 [00:05<00:09, 26.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 148/391 [00:05<00:09, 26.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▊      | 151/391 [00:06<00:09, 26.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 154/391 [00:06<00:09, 26.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|████      | 157/391 [00:06<00:08, 26.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 160/391 [00:06<00:09, 25.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 163/391 [00:06<00:08, 26.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 166/391 [00:06<00:08, 26.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 169/391 [00:06<00:08, 24.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▍     | 172/391 [00:06<00:09, 23.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▍     | 175/391 [00:06<00:09, 23.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 178/391 [00:07<00:09, 23.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▋     | 181/391 [00:07<00:08, 23.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 184/391 [00:07<00:09, 22.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 187/391 [00:07<00:09, 21.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▊     | 190/391 [00:07<00:09, 21.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 193/391 [00:07<00:09, 20.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|█████     | 196/391 [00:07<00:09, 21.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 199/391 [00:08<00:08, 21.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 202/391 [00:08<00:08, 21.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 205/391 [00:08<00:08, 21.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 208/391 [00:08<00:08, 21.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 211/391 [00:08<00:08, 22.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▍    | 214/391 [00:08<00:07, 22.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▌    | 217/391 [00:08<00:07, 22.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▋    | 220/391 [00:09<00:07, 22.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 223/391 [00:09<00:07, 21.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 226/391 [00:09<00:07, 21.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▊    | 229/391 [00:09<00:07, 20.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 232/391 [00:09<00:07, 21.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|██████    | 235/391 [00:09<00:07, 21.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 238/391 [00:09<00:07, 21.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 241/391 [00:10<00:07, 20.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 244/391 [00:10<00:07, 20.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 247/391 [00:10<00:07, 20.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 250/391 [00:10<00:06, 21.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▍   | 253/391 [00:10<00:06, 22.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▌   | 256/391 [00:10<00:05, 23.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 259/391 [00:10<00:05, 24.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 262/391 [00:10<00:05, 24.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 265/391 [00:11<00:04, 25.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▊   | 268/391 [00:11<00:04, 25.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 271/391 [00:11<00:04, 25.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|███████   | 274/391 [00:11<00:04, 25.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 277/391 [00:11<00:04, 25.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 280/391 [00:11<00:04, 25.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 283/391 [00:11<00:04, 26.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 286/391 [00:11<00:04, 26.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 289/391 [00:11<00:03, 26.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▍  | 292/391 [00:12<00:03, 26.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▌  | 295/391 [00:12<00:03, 26.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 298/391 [00:12<00:03, 25.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 301/391 [00:12<00:03, 25.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 304/391 [00:12<00:03, 25.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▊  | 307/391 [00:12<00:03, 26.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 310/391 [00:12<00:03, 26.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|████████  | 313/391 [00:12<00:03, 25.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 316/391 [00:13<00:02, 25.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 319/391 [00:13<00:02, 25.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 322/391 [00:13<00:02, 26.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 325/391 [00:13<00:02, 25.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 328/391 [00:13<00:02, 25.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▍ | 331/391 [00:13<00:02, 25.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▌ | 334/391 [00:13<00:02, 26.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 337/391 [00:13<00:02, 26.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 340/391 [00:13<00:01, 25.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 343/391 [00:14<00:01, 25.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 346/391 [00:14<00:01, 25.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▉ | 349/391 [00:14<00:01, 25.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|█████████ | 352/391 [00:14<00:01, 25.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 355/391 [00:14<00:01, 25.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 358/391 [00:14<00:01, 25.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 361/391 [00:14<00:01, 25.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 364/391 [00:14<00:01, 25.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 367/391 [00:15<00:00, 25.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▍| 370/391 [00:15<00:00, 25.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▌| 373/391 [00:15<00:00, 26.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 376/391 [00:15<00:00, 25.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 379/391 [00:15<00:00, 25.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 382/391 [00:15<00:00, 25.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 385/391 [00:15<00:00, 25.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▉| 388/391 [00:15<00:00, 26.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 391/391 [00:15<00:00, 24.54batch/s]\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/196 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3%|▎         | 6/196 [00:00<00:03, 54.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   6%|▌         | 12/196 [00:00<00:03, 54.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   9%|▉         | 18/196 [00:00<00:03, 53.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▏        | 24/196 [00:00<00:03, 49.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15%|█▌        | 30/196 [00:00<00:03, 51.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18%|█▊        | 36/196 [00:00<00:03, 52.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  21%|██▏       | 42/196 [00:00<00:02, 53.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  24%|██▍       | 48/196 [00:00<00:02, 53.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28%|██▊       | 54/196 [00:01<00:02, 52.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  31%|███       | 60/196 [00:01<00:02, 53.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  34%|███▎      | 66/196 [00:01<00:02, 54.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37%|███▋      | 72/196 [00:01<00:02, 53.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40%|███▉      | 78/196 [00:01<00:02, 51.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43%|████▎     | 84/196 [00:01<00:02, 51.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  46%|████▌     | 90/196 [00:01<00:02, 52.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  49%|████▉     | 96/196 [00:01<00:01, 53.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  52%|█████▏    | 102/196 [00:01<00:01, 52.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 108/196 [00:02<00:01, 51.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 114/196 [00:02<00:01, 51.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 120/196 [00:02<00:01, 52.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 126/196 [00:02<00:01, 50.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 132/196 [00:02<00:01, 50.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 138/196 [00:02<00:01, 51.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 144/196 [00:02<00:01, 51.00batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 150/196 [00:02<00:00, 50.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 156/196 [00:03<00:00, 50.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 162/196 [00:03<00:00, 45.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 167/196 [00:03<00:00, 46.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 172/196 [00:03<00:00, 45.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 177/196 [00:03<00:00, 44.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 183/196 [00:03<00:00, 46.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 189/196 [00:03<00:00, 48.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 50.93batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "Train Loss: 0.31 | Train Accuracy: 89.91%\n",
            "Validation Loss: 0.38 | Validation Accuracy: 85.04%\n",
            "Epoch Time: 19.88 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/391 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 3/391 [00:00<00:17, 21.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 6/391 [00:00<00:16, 23.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 9/391 [00:00<00:15, 24.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 12/391 [00:00<00:17, 22.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 15/391 [00:00<00:17, 22.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▍         | 18/391 [00:00<00:16, 22.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 21/391 [00:00<00:15, 23.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 24/391 [00:01<00:15, 23.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 27/391 [00:01<00:15, 23.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 30/391 [00:01<00:15, 22.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 33/391 [00:01<00:16, 21.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 36/391 [00:01<00:16, 21.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 39/391 [00:01<00:16, 21.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 42/391 [00:01<00:15, 22.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 45/391 [00:02<00:15, 22.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 48/391 [00:02<00:14, 22.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 51/391 [00:02<00:14, 22.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 54/391 [00:02<00:14, 23.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▍        | 57/391 [00:02<00:14, 22.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 60/391 [00:02<00:14, 22.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 63/391 [00:02<00:14, 21.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 66/391 [00:02<00:14, 22.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 69/391 [00:03<00:14, 21.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 72/391 [00:03<00:14, 21.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 75/391 [00:03<00:15, 21.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 78/391 [00:03<00:15, 20.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 81/391 [00:03<00:14, 21.23batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██▏       | 84/391 [00:03<00:14, 21.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 87/391 [00:03<00:13, 22.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 90/391 [00:04<00:14, 21.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 93/391 [00:04<00:14, 21.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▍       | 96/391 [00:04<00:13, 22.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▌       | 99/391 [00:04<00:12, 22.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 102/391 [00:04<00:12, 23.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 105/391 [00:04<00:11, 23.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 108/391 [00:04<00:11, 24.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 111/391 [00:04<00:11, 24.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 114/391 [00:05<00:12, 22.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|██▉       | 117/391 [00:05<00:11, 23.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 120/391 [00:05<00:11, 24.00batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███▏      | 123/391 [00:05<00:11, 23.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 126/391 [00:05<00:10, 24.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 129/391 [00:05<00:10, 24.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 132/391 [00:05<00:10, 24.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▍      | 135/391 [00:05<00:10, 25.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▌      | 138/391 [00:06<00:09, 25.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 141/391 [00:06<00:09, 25.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 144/391 [00:06<00:09, 26.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 147/391 [00:06<00:09, 26.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 150/391 [00:06<00:09, 25.49batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 153/391 [00:06<00:09, 25.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|███▉      | 156/391 [00:06<00:09, 25.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 159/391 [00:06<00:09, 25.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████▏     | 162/391 [00:06<00:08, 25.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 165/391 [00:07<00:08, 26.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 168/391 [00:07<00:08, 26.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▎     | 171/391 [00:07<00:08, 26.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▍     | 174/391 [00:07<00:08, 24.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▌     | 177/391 [00:07<00:08, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 180/391 [00:07<00:08, 25.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 183/391 [00:07<00:08, 25.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 186/391 [00:07<00:07, 25.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 189/391 [00:08<00:07, 26.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 192/391 [00:08<00:07, 26.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|████▉     | 195/391 [00:08<00:07, 25.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 198/391 [00:08<00:07, 25.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████▏    | 201/391 [00:08<00:07, 24.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 204/391 [00:08<00:07, 24.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 207/391 [00:08<00:07, 25.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▎    | 210/391 [00:08<00:07, 25.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 213/391 [00:08<00:06, 25.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▌    | 216/391 [00:09<00:06, 25.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 219/391 [00:09<00:06, 26.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 222/391 [00:09<00:06, 26.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 225/391 [00:09<00:06, 26.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 228/391 [00:09<00:06, 25.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 231/391 [00:09<00:06, 25.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|█████▉    | 234/391 [00:09<00:06, 25.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 237/391 [00:09<00:05, 26.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████▏   | 240/391 [00:09<00:05, 26.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 243/391 [00:10<00:05, 26.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 246/391 [00:10<00:05, 26.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▎   | 249/391 [00:10<00:05, 26.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 252/391 [00:10<00:05, 25.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▌   | 255/391 [00:10<00:05, 24.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 258/391 [00:10<00:05, 25.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 261/391 [00:10<00:05, 25.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 264/391 [00:10<00:04, 25.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 267/391 [00:11<00:04, 26.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 270/391 [00:11<00:04, 26.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|██████▉   | 273/391 [00:11<00:04, 26.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 276/391 [00:11<00:04, 26.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████▏  | 279/391 [00:11<00:04, 25.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 282/391 [00:11<00:04, 25.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 285/391 [00:11<00:04, 26.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▎  | 288/391 [00:11<00:03, 25.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 291/391 [00:11<00:03, 26.12batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▌  | 294/391 [00:12<00:03, 26.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 297/391 [00:12<00:03, 26.00batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 300/391 [00:12<00:03, 26.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 303/391 [00:12<00:03, 24.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 306/391 [00:12<00:03, 24.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 309/391 [00:12<00:03, 24.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|███████▉  | 312/391 [00:12<00:03, 25.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 315/391 [00:12<00:02, 25.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████▏ | 318/391 [00:13<00:02, 25.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 321/391 [00:13<00:02, 26.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 324/391 [00:13<00:02, 26.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▎ | 327/391 [00:13<00:02, 26.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 330/391 [00:13<00:02, 25.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▌ | 333/391 [00:13<00:02, 24.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 336/391 [00:13<00:02, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 339/391 [00:13<00:02, 25.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 342/391 [00:13<00:01, 24.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 345/391 [00:14<00:01, 25.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▉ | 348/391 [00:14<00:01, 25.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|████████▉ | 351/391 [00:14<00:01, 23.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 354/391 [00:14<00:01, 22.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████▏| 357/391 [00:14<00:01, 22.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 360/391 [00:14<00:01, 21.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 363/391 [00:14<00:01, 22.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▎| 366/391 [00:15<00:01, 22.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 369/391 [00:15<00:00, 22.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▌| 372/391 [00:15<00:00, 21.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 375/391 [00:15<00:00, 20.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 378/391 [00:15<00:00, 21.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 381/391 [00:15<00:00, 21.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 384/391 [00:15<00:00, 21.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▉| 387/391 [00:16<00:00, 21.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 391/391 [00:16<00:00, 24.14batch/s]\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/196 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3%|▎         | 5/196 [00:00<00:04, 44.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   5%|▌         | 10/196 [00:00<00:04, 37.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   7%|▋         | 14/196 [00:00<00:05, 33.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10%|▉         | 19/196 [00:00<00:04, 37.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▏        | 23/196 [00:00<00:04, 35.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  14%|█▍        | 27/196 [00:00<00:04, 35.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  16%|█▌        | 31/196 [00:00<00:04, 35.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18%|█▊        | 35/196 [00:00<00:04, 36.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  20%|█▉        | 39/196 [00:01<00:04, 37.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  22%|██▏       | 43/196 [00:01<00:04, 37.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  24%|██▍       | 47/196 [00:01<00:04, 35.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  26%|██▌       | 51/196 [00:01<00:03, 36.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  29%|██▊       | 56/196 [00:01<00:03, 37.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  31%|███       | 60/196 [00:01<00:03, 36.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  33%|███▎      | 65/196 [00:01<00:03, 38.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  35%|███▌      | 69/196 [00:01<00:03, 38.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37%|███▋      | 73/196 [00:01<00:03, 36.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  39%|███▉      | 77/196 [00:02<00:03, 34.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  41%|████▏     | 81/196 [00:02<00:03, 33.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  44%|████▍     | 86/196 [00:02<00:03, 36.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  46%|████▋     | 91/196 [00:02<00:02, 39.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  49%|████▉     | 96/196 [00:02<00:02, 42.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  52%|█████▏    | 101/196 [00:02<00:02, 42.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▍    | 106/196 [00:02<00:02, 44.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 111/196 [00:02<00:01, 45.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 116/196 [00:02<00:01, 46.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 121/196 [00:03<00:01, 46.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 126/196 [00:03<00:01, 45.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 131/196 [00:03<00:01, 44.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 136/196 [00:03<00:01, 45.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 141/196 [00:03<00:01, 46.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 147/196 [00:03<00:01, 47.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 152/196 [00:03<00:00, 45.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|████████  | 157/196 [00:03<00:00, 45.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 163/196 [00:04<00:00, 47.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 169/196 [00:04<00:00, 48.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 174/196 [00:04<00:00, 48.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 179/196 [00:04<00:00, 46.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 184/196 [00:04<00:00, 47.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 190/196 [00:04<00:00, 48.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 196/196 [00:04<00:00, 41.84batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10\n",
            "Train Loss: 0.21 | Train Accuracy: 92.14%\n",
            "Validation Loss: 0.38 | Validation Accuracy: 85.14%\n",
            "Epoch Time: 20.90 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/391 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 3/391 [00:00<00:17, 22.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 6/391 [00:00<00:15, 24.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 9/391 [00:00<00:15, 24.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 12/391 [00:00<00:15, 24.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 15/391 [00:00<00:15, 24.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▍         | 18/391 [00:00<00:15, 24.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 21/391 [00:00<00:14, 25.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 24/391 [00:00<00:14, 25.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 27/391 [00:01<00:14, 25.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 30/391 [00:01<00:14, 25.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 33/391 [00:01<00:13, 25.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 36/391 [00:01<00:13, 25.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 39/391 [00:01<00:14, 24.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 42/391 [00:01<00:13, 25.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 45/391 [00:01<00:13, 25.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 48/391 [00:01<00:13, 25.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 51/391 [00:02<00:13, 25.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 54/391 [00:02<00:13, 25.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▍        | 57/391 [00:02<00:13, 25.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 60/391 [00:02<00:13, 25.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 63/391 [00:02<00:12, 25.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 66/391 [00:02<00:13, 24.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 69/391 [00:02<00:12, 25.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 72/391 [00:02<00:12, 25.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 75/391 [00:02<00:12, 26.25batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 78/391 [00:03<00:11, 26.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 81/391 [00:03<00:12, 25.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██▏       | 84/391 [00:03<00:12, 25.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 87/391 [00:03<00:11, 25.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 90/391 [00:03<00:11, 25.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 93/391 [00:03<00:11, 25.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▍       | 96/391 [00:03<00:11, 25.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▌       | 99/391 [00:03<00:11, 25.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 102/391 [00:04<00:11, 25.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 105/391 [00:04<00:11, 25.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 108/391 [00:04<00:11, 25.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 111/391 [00:04<00:10, 25.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 114/391 [00:04<00:10, 25.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|██▉       | 117/391 [00:04<00:10, 25.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 120/391 [00:04<00:10, 25.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███▏      | 123/391 [00:04<00:10, 25.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 126/391 [00:04<00:10, 26.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 129/391 [00:05<00:10, 26.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 132/391 [00:05<00:09, 25.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▍      | 135/391 [00:05<00:09, 25.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▌      | 138/391 [00:05<00:09, 25.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 141/391 [00:05<00:09, 25.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 144/391 [00:05<00:09, 25.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 147/391 [00:05<00:09, 25.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 150/391 [00:05<00:09, 25.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 153/391 [00:06<00:09, 25.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|███▉      | 156/391 [00:06<00:08, 26.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 159/391 [00:06<00:09, 25.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████▏     | 162/391 [00:06<00:08, 25.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 165/391 [00:06<00:08, 25.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 168/391 [00:06<00:08, 25.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▎     | 171/391 [00:06<00:08, 24.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▍     | 174/391 [00:06<00:08, 25.00batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▌     | 177/391 [00:06<00:08, 25.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 180/391 [00:07<00:08, 25.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 183/391 [00:07<00:08, 25.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 186/391 [00:07<00:08, 25.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 189/391 [00:07<00:07, 25.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 192/391 [00:07<00:08, 23.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|████▉     | 195/391 [00:07<00:08, 22.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 198/391 [00:07<00:08, 22.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████▏    | 201/391 [00:07<00:08, 22.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 204/391 [00:08<00:08, 22.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 207/391 [00:08<00:08, 22.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▎    | 210/391 [00:08<00:07, 22.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 213/391 [00:08<00:08, 21.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▌    | 216/391 [00:08<00:08, 20.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 219/391 [00:08<00:08, 21.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 222/391 [00:08<00:08, 20.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 225/391 [00:09<00:07, 21.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 228/391 [00:09<00:07, 22.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 231/391 [00:09<00:07, 21.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|█████▉    | 234/391 [00:09<00:07, 21.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 237/391 [00:09<00:07, 21.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████▏   | 240/391 [00:09<00:06, 22.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 243/391 [00:09<00:06, 21.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 246/391 [00:10<00:06, 21.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▎   | 249/391 [00:10<00:06, 21.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 252/391 [00:10<00:06, 21.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▌   | 255/391 [00:10<00:06, 20.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 258/391 [00:10<00:06, 20.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 261/391 [00:10<00:06, 20.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 264/391 [00:10<00:06, 20.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 267/391 [00:11<00:06, 20.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 270/391 [00:11<00:06, 19.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|██████▉   | 273/391 [00:11<00:05, 20.26batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 276/391 [00:11<00:05, 21.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████▏  | 279/391 [00:11<00:04, 22.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 282/391 [00:11<00:04, 23.23batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 285/391 [00:11<00:04, 23.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▎  | 288/391 [00:12<00:04, 24.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 291/391 [00:12<00:04, 24.26batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▌  | 294/391 [00:12<00:03, 24.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 297/391 [00:12<00:03, 24.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 300/391 [00:12<00:03, 24.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 303/391 [00:12<00:03, 24.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 306/391 [00:12<00:03, 24.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 309/391 [00:12<00:03, 24.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|███████▉  | 312/391 [00:12<00:03, 24.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 315/391 [00:13<00:03, 24.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████▏ | 318/391 [00:13<00:02, 24.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 321/391 [00:13<00:02, 25.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 324/391 [00:13<00:02, 25.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▎ | 327/391 [00:13<00:02, 24.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 330/391 [00:13<00:02, 24.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▌ | 333/391 [00:13<00:02, 24.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 336/391 [00:13<00:02, 25.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 339/391 [00:14<00:02, 25.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 342/391 [00:14<00:01, 25.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 345/391 [00:14<00:01, 25.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▉ | 348/391 [00:14<00:01, 24.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|████████▉ | 351/391 [00:14<00:01, 24.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 354/391 [00:14<00:01, 24.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████▏| 357/391 [00:14<00:01, 24.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 360/391 [00:14<00:01, 24.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 363/391 [00:15<00:01, 24.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▎| 366/391 [00:15<00:01, 24.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 369/391 [00:15<00:00, 25.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▌| 372/391 [00:15<00:00, 24.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 375/391 [00:15<00:00, 24.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 378/391 [00:15<00:00, 24.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 381/391 [00:15<00:00, 24.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 384/391 [00:15<00:00, 24.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▉| 387/391 [00:15<00:00, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 391/391 [00:16<00:00, 24.23batch/s]\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/196 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3%|▎         | 6/196 [00:00<00:03, 56.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   6%|▌         | 12/196 [00:00<00:03, 53.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   9%|▉         | 18/196 [00:00<00:03, 51.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▏        | 24/196 [00:00<00:03, 48.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15%|█▌        | 30/196 [00:00<00:03, 49.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18%|█▊        | 36/196 [00:00<00:03, 50.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  21%|██▏       | 42/196 [00:00<00:03, 50.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  24%|██▍       | 48/196 [00:00<00:02, 50.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28%|██▊       | 54/196 [00:01<00:02, 52.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  31%|███       | 60/196 [00:01<00:02, 52.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  34%|███▎      | 66/196 [00:01<00:02, 53.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37%|███▋      | 72/196 [00:01<00:02, 50.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40%|███▉      | 78/196 [00:01<00:02, 49.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43%|████▎     | 84/196 [00:01<00:02, 50.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  46%|████▌     | 90/196 [00:01<00:02, 51.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  49%|████▉     | 96/196 [00:01<00:01, 52.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  52%|█████▏    | 102/196 [00:01<00:01, 52.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 108/196 [00:02<00:01, 52.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 114/196 [00:02<00:01, 52.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 120/196 [00:02<00:01, 51.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 126/196 [00:02<00:01, 47.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 132/196 [00:02<00:01, 49.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 138/196 [00:02<00:01, 50.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 144/196 [00:02<00:01, 51.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 150/196 [00:02<00:00, 51.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 156/196 [00:03<00:00, 50.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 162/196 [00:03<00:00, 49.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▌ | 167/196 [00:03<00:00, 49.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 172/196 [00:03<00:00, 47.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 177/196 [00:03<00:00, 45.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 183/196 [00:03<00:00, 46.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 189/196 [00:03<00:00, 48.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 50.44batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10\n",
            "Train Loss: 0.18 | Train Accuracy: 93.31%\n",
            "Validation Loss: 0.39 | Validation Accuracy: 85.30%\n",
            "Epoch Time: 20.05 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/391 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 3/391 [00:00<00:15, 25.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 6/391 [00:00<00:15, 25.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 9/391 [00:00<00:15, 25.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 12/391 [00:00<00:15, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 15/391 [00:00<00:15, 24.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▍         | 18/391 [00:00<00:14, 24.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 21/391 [00:00<00:14, 25.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 24/391 [00:00<00:14, 25.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 27/391 [00:01<00:14, 24.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 30/391 [00:01<00:14, 25.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 33/391 [00:01<00:14, 24.78batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 36/391 [00:01<00:15, 23.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 39/391 [00:01<00:16, 21.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 42/391 [00:01<00:15, 21.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 45/391 [00:01<00:15, 22.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 48/391 [00:02<00:15, 22.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 51/391 [00:02<00:14, 22.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 54/391 [00:02<00:15, 22.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▍        | 57/391 [00:02<00:15, 21.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 60/391 [00:02<00:16, 20.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 63/391 [00:02<00:15, 20.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 66/391 [00:02<00:15, 21.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 69/391 [00:03<00:14, 21.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 72/391 [00:03<00:14, 21.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 75/391 [00:03<00:14, 22.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 78/391 [00:03<00:13, 22.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 81/391 [00:03<00:14, 21.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██▏       | 84/391 [00:03<00:15, 20.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 87/391 [00:03<00:14, 20.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 90/391 [00:04<00:14, 20.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 93/391 [00:04<00:14, 20.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▍       | 96/391 [00:04<00:14, 20.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▌       | 99/391 [00:04<00:13, 20.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 102/391 [00:04<00:14, 19.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 105/391 [00:04<00:14, 20.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 108/391 [00:04<00:14, 19.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 110/391 [00:05<00:14, 19.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 113/391 [00:05<00:13, 20.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|██▉       | 116/391 [00:05<00:12, 22.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|███       | 119/391 [00:05<00:11, 23.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 122/391 [00:05<00:11, 23.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 125/391 [00:05<00:11, 23.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 128/391 [00:05<00:10, 24.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▎      | 131/391 [00:05<00:10, 24.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 134/391 [00:05<00:10, 24.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▌      | 137/391 [00:06<00:10, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 140/391 [00:06<00:10, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 143/391 [00:06<00:09, 25.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 146/391 [00:06<00:09, 25.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 149/391 [00:06<00:09, 24.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 152/391 [00:06<00:09, 24.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|███▉      | 155/391 [00:06<00:09, 25.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|████      | 158/391 [00:06<00:09, 24.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 161/391 [00:07<00:09, 25.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 164/391 [00:07<00:08, 25.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 167/391 [00:07<00:08, 25.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 170/391 [00:07<00:08, 25.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▍     | 173/391 [00:07<00:08, 24.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▌     | 176/391 [00:07<00:08, 24.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 179/391 [00:07<00:08, 25.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 182/391 [00:07<00:08, 24.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 185/391 [00:08<00:08, 24.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 188/391 [00:08<00:08, 24.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 191/391 [00:08<00:08, 24.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|████▉     | 194/391 [00:08<00:07, 24.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|█████     | 197/391 [00:08<00:07, 24.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 200/391 [00:08<00:07, 24.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 203/391 [00:08<00:07, 24.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 206/391 [00:08<00:07, 25.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 209/391 [00:08<00:07, 24.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 212/391 [00:09<00:07, 25.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▍    | 215/391 [00:09<00:07, 25.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 218/391 [00:09<00:06, 25.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 221/391 [00:09<00:06, 25.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 224/391 [00:09<00:06, 25.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 227/391 [00:09<00:06, 25.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 230/391 [00:09<00:06, 25.12batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|█████▉    | 233/391 [00:09<00:06, 24.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|██████    | 236/391 [00:10<00:06, 24.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 239/391 [00:10<00:06, 25.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 242/391 [00:10<00:05, 25.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 245/391 [00:10<00:05, 25.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 248/391 [00:10<00:05, 24.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 251/391 [00:10<00:05, 24.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▍   | 254/391 [00:10<00:05, 25.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 257/391 [00:10<00:05, 25.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▋   | 260/391 [00:11<00:05, 24.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 263/391 [00:11<00:05, 24.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 266/391 [00:11<00:04, 25.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 269/391 [00:11<00:04, 25.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|██████▉   | 272/391 [00:11<00:04, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|███████   | 275/391 [00:11<00:04, 24.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 278/391 [00:11<00:04, 25.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 281/391 [00:11<00:04, 25.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 284/391 [00:11<00:04, 25.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 287/391 [00:12<00:04, 25.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 290/391 [00:12<00:04, 25.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▍  | 293/391 [00:12<00:03, 25.26batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 296/391 [00:12<00:03, 25.26batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▋  | 299/391 [00:12<00:03, 25.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 302/391 [00:12<00:03, 25.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 305/391 [00:12<00:03, 25.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 308/391 [00:12<00:03, 25.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|███████▉  | 311/391 [00:13<00:03, 24.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|████████  | 314/391 [00:13<00:03, 25.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 317/391 [00:13<00:02, 25.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 320/391 [00:13<00:02, 25.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 323/391 [00:13<00:02, 24.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 326/391 [00:13<00:02, 24.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 329/391 [00:13<00:02, 24.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▍ | 332/391 [00:13<00:02, 25.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 335/391 [00:14<00:02, 25.25batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▋ | 338/391 [00:14<00:02, 25.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 341/391 [00:14<00:01, 25.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 344/391 [00:14<00:01, 25.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▊ | 347/391 [00:14<00:01, 24.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|████████▉ | 350/391 [00:14<00:01, 24.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|█████████ | 353/391 [00:14<00:01, 24.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 356/391 [00:14<00:01, 25.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 359/391 [00:14<00:01, 25.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 362/391 [00:15<00:01, 24.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 365/391 [00:15<00:01, 23.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 368/391 [00:15<00:01, 22.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▍| 371/391 [00:15<00:00, 21.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 374/391 [00:15<00:00, 21.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▋| 377/391 [00:15<00:00, 21.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 380/391 [00:15<00:00, 21.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 383/391 [00:16<00:00, 20.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▊| 386/391 [00:16<00:00, 20.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 391/391 [00:16<00:00, 23.68batch/s]\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/196 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   2%|▏         | 4/196 [00:00<00:04, 38.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   5%|▍         | 9/196 [00:00<00:04, 40.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   7%|▋         | 14/196 [00:00<00:04, 41.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10%|▉         | 19/196 [00:00<00:04, 42.12batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▏        | 24/196 [00:00<00:03, 43.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15%|█▍        | 29/196 [00:00<00:04, 41.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  17%|█▋        | 34/196 [00:00<00:04, 39.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  19%|█▉        | 38/196 [00:00<00:04, 36.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  21%|██▏       | 42/196 [00:01<00:04, 36.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  23%|██▎       | 46/196 [00:01<00:04, 35.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  26%|██▌       | 50/196 [00:01<00:04, 35.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28%|██▊       | 54/196 [00:01<00:03, 35.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  30%|██▉       | 58/196 [00:01<00:03, 35.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  32%|███▏      | 62/196 [00:01<00:03, 35.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  34%|███▎      | 66/196 [00:01<00:03, 35.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  36%|███▌      | 70/196 [00:01<00:03, 35.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  38%|███▊      | 74/196 [00:02<00:03, 33.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40%|███▉      | 78/196 [00:02<00:03, 32.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  42%|████▏     | 82/196 [00:02<00:03, 32.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  44%|████▍     | 86/196 [00:02<00:03, 31.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  46%|████▌     | 90/196 [00:02<00:03, 33.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48%|████▊     | 95/196 [00:02<00:02, 36.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  51%|█████     | 100/196 [00:02<00:02, 39.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 105/196 [00:02<00:02, 40.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 110/196 [00:02<00:02, 41.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 115/196 [00:03<00:01, 41.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 120/196 [00:03<00:01, 42.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 125/196 [00:03<00:01, 44.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 130/196 [00:03<00:01, 45.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 135/196 [00:03<00:01, 45.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 140/196 [00:03<00:01, 45.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 145/196 [00:03<00:01, 45.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 150/196 [00:03<00:01, 45.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 155/196 [00:03<00:00, 44.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 160/196 [00:04<00:00, 44.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 165/196 [00:04<00:00, 44.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 170/196 [00:04<00:00, 45.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 175/196 [00:04<00:00, 46.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 180/196 [00:04<00:00, 46.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 185/196 [00:04<00:00, 47.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 190/196 [00:04<00:00, 46.26batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 196/196 [00:04<00:00, 40.58batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10\n",
            "Train Loss: 0.16 | Train Accuracy: 94.25%\n",
            "Validation Loss: 0.41 | Validation Accuracy: 85.53%\n",
            "Epoch Time: 21.37 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/391 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 3/391 [00:00<00:18, 20.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 6/391 [00:00<00:18, 21.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 9/391 [00:00<00:16, 22.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 12/391 [00:00<00:16, 23.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 15/391 [00:00<00:15, 23.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▍         | 18/391 [00:00<00:15, 24.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 21/391 [00:00<00:15, 24.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 24/391 [00:01<00:14, 24.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 27/391 [00:01<00:14, 24.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 30/391 [00:01<00:14, 24.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 33/391 [00:01<00:14, 24.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 36/391 [00:01<00:14, 24.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 39/391 [00:01<00:13, 25.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 42/391 [00:01<00:13, 25.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 45/391 [00:01<00:13, 25.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 48/391 [00:01<00:13, 25.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 51/391 [00:02<00:13, 24.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 54/391 [00:02<00:13, 24.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▍        | 57/391 [00:02<00:13, 24.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 60/391 [00:02<00:13, 24.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 63/391 [00:02<00:13, 24.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 66/391 [00:02<00:13, 24.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 69/391 [00:02<00:12, 25.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 72/391 [00:02<00:12, 25.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 75/391 [00:03<00:12, 25.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 78/391 [00:03<00:12, 24.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 81/391 [00:03<00:12, 24.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██▏       | 84/391 [00:03<00:12, 25.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 87/391 [00:03<00:12, 25.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 90/391 [00:03<00:11, 25.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 93/391 [00:03<00:11, 25.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▍       | 96/391 [00:03<00:11, 25.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▌       | 99/391 [00:03<00:11, 25.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 102/391 [00:04<00:11, 24.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 105/391 [00:04<00:11, 24.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 108/391 [00:04<00:11, 24.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 111/391 [00:04<00:11, 25.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 114/391 [00:04<00:11, 25.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|██▉       | 117/391 [00:04<00:11, 24.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 120/391 [00:04<00:10, 25.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███▏      | 123/391 [00:04<00:10, 25.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 126/391 [00:05<00:10, 25.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 129/391 [00:05<00:10, 24.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 132/391 [00:05<00:10, 24.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▍      | 135/391 [00:05<00:10, 24.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▌      | 138/391 [00:05<00:10, 25.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 141/391 [00:05<00:09, 25.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 144/391 [00:05<00:09, 25.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 147/391 [00:05<00:09, 25.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 150/391 [00:06<00:09, 25.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 153/391 [00:06<00:09, 23.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|███▉      | 156/391 [00:06<00:09, 24.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 159/391 [00:06<00:09, 24.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████▏     | 162/391 [00:06<00:09, 25.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 165/391 [00:06<00:08, 25.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 168/391 [00:06<00:08, 25.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▎     | 171/391 [00:06<00:08, 25.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▍     | 174/391 [00:07<00:08, 25.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▌     | 177/391 [00:07<00:08, 24.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 180/391 [00:07<00:08, 24.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 183/391 [00:07<00:08, 24.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 186/391 [00:07<00:08, 25.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 189/391 [00:07<00:08, 25.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 192/391 [00:07<00:08, 22.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|████▉     | 195/391 [00:07<00:08, 22.78batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 198/391 [00:08<00:08, 22.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████▏    | 201/391 [00:08<00:08, 22.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 204/391 [00:08<00:08, 21.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 207/391 [00:08<00:08, 21.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▎    | 210/391 [00:08<00:08, 21.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 213/391 [00:08<00:08, 21.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▌    | 216/391 [00:08<00:08, 21.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 219/391 [00:09<00:08, 21.25batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 222/391 [00:09<00:08, 20.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 225/391 [00:09<00:07, 21.00batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 228/391 [00:09<00:07, 21.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 231/391 [00:09<00:07, 21.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|█████▉    | 234/391 [00:09<00:07, 21.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 237/391 [00:09<00:07, 21.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████▏   | 240/391 [00:10<00:07, 21.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 243/391 [00:10<00:07, 20.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 246/391 [00:10<00:07, 19.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▎   | 249/391 [00:10<00:07, 20.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 252/391 [00:10<00:06, 20.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▌   | 255/391 [00:10<00:06, 19.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 258/391 [00:10<00:06, 20.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 261/391 [00:11<00:06, 20.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 264/391 [00:11<00:06, 20.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 267/391 [00:11<00:06, 20.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 270/391 [00:11<00:05, 20.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|██████▉   | 273/391 [00:11<00:05, 20.25batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 276/391 [00:11<00:05, 20.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████▏  | 279/391 [00:11<00:05, 20.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 282/391 [00:12<00:05, 18.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 285/391 [00:12<00:05, 19.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▎  | 288/391 [00:12<00:04, 21.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 291/391 [00:12<00:04, 21.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▌  | 294/391 [00:12<00:04, 22.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 297/391 [00:12<00:04, 23.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 300/391 [00:12<00:03, 23.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 303/391 [00:13<00:03, 24.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 306/391 [00:13<00:03, 23.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 309/391 [00:13<00:03, 24.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|███████▉  | 312/391 [00:13<00:03, 24.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 315/391 [00:13<00:03, 23.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████▏ | 318/391 [00:13<00:03, 24.12batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 321/391 [00:13<00:02, 24.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 324/391 [00:13<00:02, 24.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▎ | 327/391 [00:13<00:02, 24.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 330/391 [00:14<00:02, 24.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▌ | 333/391 [00:14<00:02, 24.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 336/391 [00:14<00:02, 24.49batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 339/391 [00:14<00:02, 24.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 342/391 [00:14<00:01, 24.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 345/391 [00:14<00:01, 24.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▉ | 348/391 [00:14<00:01, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|████████▉ | 351/391 [00:14<00:01, 24.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 354/391 [00:15<00:01, 24.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████▏| 357/391 [00:15<00:01, 24.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 360/391 [00:15<00:01, 24.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 363/391 [00:15<00:01, 24.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▎| 366/391 [00:15<00:01, 24.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 369/391 [00:15<00:00, 24.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▌| 372/391 [00:15<00:00, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 375/391 [00:15<00:00, 24.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 378/391 [00:16<00:00, 24.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 381/391 [00:16<00:00, 23.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 384/391 [00:16<00:00, 24.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▉| 387/391 [00:16<00:00, 24.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 391/391 [00:16<00:00, 23.56batch/s]\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/196 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3%|▎         | 6/196 [00:00<00:03, 54.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   6%|▌         | 12/196 [00:00<00:03, 52.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   9%|▉         | 18/196 [00:00<00:03, 53.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▏        | 24/196 [00:00<00:03, 52.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15%|█▌        | 30/196 [00:00<00:03, 48.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18%|█▊        | 36/196 [00:00<00:03, 49.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  21%|██▏       | 42/196 [00:00<00:03, 49.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  24%|██▍       | 48/196 [00:00<00:02, 49.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28%|██▊       | 54/196 [00:01<00:02, 50.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  31%|███       | 60/196 [00:01<00:02, 50.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  34%|███▎      | 66/196 [00:01<00:02, 50.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37%|███▋      | 72/196 [00:01<00:02, 51.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40%|███▉      | 78/196 [00:01<00:02, 49.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43%|████▎     | 84/196 [00:01<00:02, 50.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  46%|████▌     | 90/196 [00:01<00:02, 51.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  49%|████▉     | 96/196 [00:01<00:01, 51.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  52%|█████▏    | 102/196 [00:02<00:01, 50.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 108/196 [00:02<00:01, 50.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 114/196 [00:02<00:01, 50.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 120/196 [00:02<00:01, 49.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 126/196 [00:02<00:01, 48.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 131/196 [00:02<00:01, 46.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|██████▉   | 137/196 [00:02<00:01, 47.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 142/196 [00:02<00:01, 48.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  75%|███████▌  | 147/196 [00:02<00:01, 47.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 152/196 [00:03<00:00, 47.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 158/196 [00:03<00:00, 48.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 163/196 [00:03<00:00, 48.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 168/196 [00:03<00:00, 47.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 173/196 [00:03<00:00, 46.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 178/196 [00:03<00:00, 44.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 184/196 [00:03<00:00, 46.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 189/196 [00:03<00:00, 46.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 49.20batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10\n",
            "Train Loss: 0.14 | Train Accuracy: 95.17%\n",
            "Validation Loss: 0.43 | Validation Accuracy: 85.30%\n",
            "Epoch Time: 20.60 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/391 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 3/391 [00:00<00:17, 22.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 6/391 [00:00<00:16, 23.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 9/391 [00:00<00:15, 24.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 12/391 [00:00<00:15, 24.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 15/391 [00:00<00:15, 23.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▍         | 18/391 [00:00<00:15, 24.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 21/391 [00:00<00:15, 24.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 24/391 [00:00<00:14, 24.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 27/391 [00:01<00:14, 24.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 30/391 [00:01<00:14, 24.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 33/391 [00:01<00:14, 24.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 36/391 [00:01<00:14, 24.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 39/391 [00:01<00:15, 23.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 42/391 [00:01<00:15, 22.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 45/391 [00:01<00:15, 22.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 48/391 [00:02<00:15, 22.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 51/391 [00:02<00:15, 21.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 54/391 [00:02<00:15, 21.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▍        | 57/391 [00:02<00:15, 20.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 60/391 [00:02<00:16, 20.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 63/391 [00:02<00:16, 20.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 66/391 [00:02<00:15, 20.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 69/391 [00:03<00:15, 20.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 72/391 [00:03<00:15, 20.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 75/391 [00:03<00:14, 21.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 78/391 [00:03<00:14, 21.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 81/391 [00:03<00:14, 21.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██▏       | 84/391 [00:03<00:14, 21.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 87/391 [00:03<00:13, 21.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 90/391 [00:04<00:14, 20.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 93/391 [00:04<00:14, 20.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▍       | 96/391 [00:04<00:14, 20.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▌       | 99/391 [00:04<00:14, 19.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 101/391 [00:04<00:15, 19.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▋       | 103/391 [00:04<00:14, 19.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 106/391 [00:04<00:14, 19.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 108/391 [00:04<00:14, 19.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 110/391 [00:05<00:14, 19.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▊       | 112/391 [00:05<00:14, 19.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 114/391 [00:05<00:14, 18.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|██▉       | 117/391 [00:05<00:13, 19.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 120/391 [00:05<00:13, 20.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███▏      | 123/391 [00:05<00:12, 22.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 126/391 [00:05<00:11, 23.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 129/391 [00:05<00:11, 23.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 132/391 [00:06<00:10, 24.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▍      | 135/391 [00:06<00:10, 24.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▌      | 138/391 [00:06<00:10, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 141/391 [00:06<00:10, 24.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 144/391 [00:06<00:10, 23.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 147/391 [00:06<00:10, 24.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 150/391 [00:06<00:10, 23.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 153/391 [00:06<00:09, 24.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|███▉      | 156/391 [00:07<00:09, 24.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 159/391 [00:07<00:09, 24.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████▏     | 162/391 [00:07<00:09, 24.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 165/391 [00:07<00:09, 24.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 168/391 [00:07<00:09, 24.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▎     | 171/391 [00:07<00:09, 24.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▍     | 174/391 [00:07<00:08, 24.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▌     | 177/391 [00:07<00:08, 24.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 180/391 [00:08<00:08, 24.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 183/391 [00:08<00:08, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 186/391 [00:08<00:08, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 189/391 [00:08<00:08, 24.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 192/391 [00:08<00:08, 24.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|████▉     | 195/391 [00:08<00:08, 24.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 198/391 [00:08<00:07, 24.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████▏    | 201/391 [00:08<00:07, 24.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 204/391 [00:08<00:07, 24.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 207/391 [00:09<00:07, 24.78batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▎    | 210/391 [00:09<00:07, 24.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 213/391 [00:09<00:07, 24.78batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▌    | 216/391 [00:09<00:07, 24.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 219/391 [00:09<00:07, 23.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 222/391 [00:09<00:06, 24.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 225/391 [00:09<00:06, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 228/391 [00:09<00:06, 25.25batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 231/391 [00:10<00:06, 25.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|█████▉    | 234/391 [00:10<00:06, 25.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 237/391 [00:10<00:06, 25.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████▏   | 240/391 [00:10<00:06, 24.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 243/391 [00:10<00:06, 24.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 246/391 [00:10<00:05, 24.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▎   | 249/391 [00:10<00:05, 24.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 252/391 [00:10<00:05, 25.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▌   | 255/391 [00:11<00:05, 25.25batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 258/391 [00:11<00:05, 25.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 261/391 [00:11<00:05, 25.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 264/391 [00:11<00:05, 25.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 267/391 [00:11<00:05, 24.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 270/391 [00:11<00:04, 24.23batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|██████▉   | 273/391 [00:11<00:04, 24.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 276/391 [00:11<00:04, 24.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████▏  | 279/391 [00:12<00:04, 25.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 282/391 [00:12<00:04, 25.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 285/391 [00:12<00:04, 25.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▎  | 288/391 [00:12<00:04, 25.26batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 291/391 [00:12<00:04, 24.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▌  | 294/391 [00:12<00:04, 23.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 297/391 [00:12<00:03, 24.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 300/391 [00:12<00:03, 24.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 303/391 [00:12<00:03, 25.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 306/391 [00:13<00:03, 25.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 309/391 [00:13<00:03, 25.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|███████▉  | 312/391 [00:13<00:03, 24.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 315/391 [00:13<00:03, 24.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████▏ | 318/391 [00:13<00:03, 23.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 321/391 [00:13<00:02, 24.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 324/391 [00:13<00:02, 24.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▎ | 327/391 [00:13<00:02, 24.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 330/391 [00:14<00:02, 24.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▌ | 333/391 [00:14<00:02, 24.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 336/391 [00:14<00:02, 24.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 339/391 [00:14<00:02, 24.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 342/391 [00:14<00:02, 23.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 345/391 [00:14<00:01, 23.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▉ | 348/391 [00:14<00:01, 23.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|████████▉ | 351/391 [00:14<00:01, 23.96batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 354/391 [00:15<00:01, 24.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████▏| 357/391 [00:15<00:01, 24.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 360/391 [00:15<00:01, 24.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 363/391 [00:15<00:01, 23.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▎| 366/391 [00:15<00:01, 20.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 369/391 [00:15<00:01, 20.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▌| 372/391 [00:15<00:00, 21.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 375/391 [00:16<00:00, 21.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 378/391 [00:16<00:00, 21.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 381/391 [00:16<00:00, 20.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 384/391 [00:16<00:00, 19.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▊| 386/391 [00:16<00:00, 18.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 391/391 [00:16<00:00, 23.15batch/s]\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/196 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3%|▎         | 5/196 [00:00<00:04, 43.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   5%|▌         | 10/196 [00:00<00:04, 41.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   8%|▊         | 15/196 [00:00<00:04, 38.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10%|█         | 20/196 [00:00<00:04, 40.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  13%|█▎        | 25/196 [00:00<00:04, 38.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15%|█▍        | 29/196 [00:00<00:04, 38.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  17%|█▋        | 33/196 [00:00<00:04, 35.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  19%|█▉        | 37/196 [00:01<00:04, 33.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  21%|██        | 41/196 [00:01<00:04, 34.12batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  23%|██▎       | 45/196 [00:01<00:04, 33.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  25%|██▌       | 49/196 [00:01<00:04, 34.25batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  27%|██▋       | 53/196 [00:01<00:04, 34.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  29%|██▉       | 57/196 [00:01<00:04, 33.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  31%|███       | 61/196 [00:01<00:04, 32.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  33%|███▎      | 65/196 [00:01<00:03, 33.23batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  35%|███▌      | 69/196 [00:01<00:03, 33.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37%|███▋      | 73/196 [00:02<00:03, 33.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  39%|███▉      | 77/196 [00:02<00:03, 33.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  41%|████▏     | 81/196 [00:02<00:03, 32.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43%|████▎     | 85/196 [00:02<00:03, 31.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  45%|████▌     | 89/196 [00:02<00:03, 32.49batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  47%|████▋     | 93/196 [00:02<00:03, 33.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  50%|█████     | 98/196 [00:02<00:02, 36.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  53%|█████▎    | 103/196 [00:02<00:02, 38.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  55%|█████▌    | 108/196 [00:03<00:02, 40.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 113/196 [00:03<00:02, 41.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  60%|██████    | 118/196 [00:03<00:01, 42.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 123/196 [00:03<00:01, 42.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▌   | 128/196 [00:03<00:01, 43.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 133/196 [00:03<00:01, 42.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 138/196 [00:03<00:01, 41.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 143/196 [00:03<00:01, 42.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 148/196 [00:03<00:01, 42.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  78%|███████▊  | 153/196 [00:04<00:01, 42.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 158/196 [00:04<00:00, 43.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  83%|████████▎ | 163/196 [00:04<00:00, 44.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 168/196 [00:04<00:00, 44.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 173/196 [00:04<00:00, 45.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████ | 178/196 [00:04<00:00, 42.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 183/196 [00:04<00:00, 42.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▌| 188/196 [00:04<00:00, 43.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 196/196 [00:05<00:00, 38.80batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10\n",
            "Train Loss: 0.13 | Train Accuracy: 95.49%\n",
            "Validation Loss: 0.44 | Validation Accuracy: 85.46%\n",
            "Epoch Time: 21.97 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/391 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 3/391 [00:00<00:18, 20.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 6/391 [00:00<00:17, 22.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 9/391 [00:00<00:16, 23.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 12/391 [00:00<00:15, 23.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 15/391 [00:00<00:16, 23.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▍         | 18/391 [00:00<00:15, 23.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 21/391 [00:00<00:15, 24.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 24/391 [00:01<00:15, 24.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 27/391 [00:01<00:14, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 30/391 [00:01<00:14, 24.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 33/391 [00:01<00:14, 25.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 36/391 [00:01<00:14, 25.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 39/391 [00:01<00:14, 24.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 42/391 [00:01<00:14, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 45/391 [00:01<00:13, 24.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 48/391 [00:01<00:13, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 51/391 [00:02<00:13, 24.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 54/391 [00:02<00:13, 24.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▍        | 57/391 [00:02<00:13, 25.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 60/391 [00:02<00:13, 25.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 63/391 [00:02<00:13, 24.49batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 66/391 [00:02<00:13, 24.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 69/391 [00:02<00:13, 24.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 72/391 [00:02<00:12, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 75/391 [00:03<00:12, 24.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 78/391 [00:03<00:12, 24.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 81/391 [00:03<00:12, 25.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██▏       | 84/391 [00:03<00:12, 25.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 87/391 [00:03<00:12, 24.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 90/391 [00:03<00:12, 24.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 93/391 [00:03<00:12, 24.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▍       | 96/391 [00:03<00:11, 25.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▌       | 99/391 [00:04<00:11, 25.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 102/391 [00:04<00:11, 25.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 105/391 [00:04<00:11, 24.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 108/391 [00:04<00:11, 24.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 111/391 [00:04<00:11, 24.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 114/391 [00:04<00:11, 24.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|██▉       | 117/391 [00:04<00:10, 24.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 120/391 [00:04<00:10, 25.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███▏      | 123/391 [00:04<00:10, 25.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 126/391 [00:05<00:10, 24.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 129/391 [00:05<00:10, 25.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 132/391 [00:05<00:10, 25.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▍      | 135/391 [00:05<00:10, 25.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▌      | 138/391 [00:05<00:10, 24.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 141/391 [00:05<00:10, 24.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 144/391 [00:05<00:10, 24.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 147/391 [00:05<00:10, 23.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 150/391 [00:06<00:10, 24.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 153/391 [00:06<00:09, 24.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|███▉      | 156/391 [00:06<00:09, 24.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 159/391 [00:06<00:09, 24.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████▏     | 162/391 [00:06<00:09, 24.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 165/391 [00:06<00:09, 24.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 168/391 [00:06<00:08, 24.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▎     | 171/391 [00:06<00:08, 25.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▍     | 174/391 [00:07<00:08, 25.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▌     | 177/391 [00:07<00:08, 24.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 180/391 [00:07<00:08, 24.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 183/391 [00:07<00:08, 24.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 186/391 [00:07<00:09, 21.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 189/391 [00:07<00:09, 21.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 192/391 [00:07<00:09, 21.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|████▉     | 195/391 [00:08<00:08, 22.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 198/391 [00:08<00:08, 22.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████▏    | 201/391 [00:08<00:08, 22.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 204/391 [00:08<00:08, 22.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 207/391 [00:08<00:08, 20.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▎    | 210/391 [00:08<00:08, 20.23batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 213/391 [00:08<00:08, 20.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▌    | 216/391 [00:09<00:08, 20.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 219/391 [00:09<00:07, 21.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 222/391 [00:09<00:08, 21.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 225/391 [00:09<00:07, 21.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 228/391 [00:09<00:07, 21.12batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 231/391 [00:09<00:07, 20.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|█████▉    | 234/391 [00:09<00:07, 20.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 237/391 [00:10<00:07, 20.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████▏   | 240/391 [00:10<00:07, 19.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 242/391 [00:10<00:07, 19.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 245/391 [00:10<00:07, 19.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 247/391 [00:10<00:07, 19.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▎   | 249/391 [00:10<00:07, 18.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 252/391 [00:10<00:07, 19.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▌   | 255/391 [00:10<00:06, 19.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 257/391 [00:11<00:06, 19.25batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 259/391 [00:11<00:07, 18.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 261/391 [00:11<00:06, 18.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 264/391 [00:11<00:06, 20.78batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 267/391 [00:11<00:05, 21.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 270/391 [00:11<00:05, 22.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|██████▉   | 273/391 [00:11<00:05, 23.12batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 276/391 [00:11<00:04, 23.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████▏  | 279/391 [00:12<00:04, 24.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 282/391 [00:12<00:04, 24.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 285/391 [00:12<00:04, 24.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▎  | 288/391 [00:12<00:04, 24.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 291/391 [00:12<00:04, 24.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▌  | 294/391 [00:12<00:04, 24.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 297/391 [00:12<00:03, 24.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 300/391 [00:12<00:03, 24.49batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 303/391 [00:13<00:03, 24.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 306/391 [00:13<00:03, 24.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 309/391 [00:13<00:03, 24.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|███████▉  | 312/391 [00:13<00:03, 24.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 315/391 [00:13<00:03, 24.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████▏ | 318/391 [00:13<00:03, 24.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 321/391 [00:13<00:02, 24.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 324/391 [00:13<00:02, 25.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▎ | 327/391 [00:13<00:02, 25.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 330/391 [00:14<00:02, 24.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▌ | 333/391 [00:14<00:02, 24.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 336/391 [00:14<00:02, 24.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 339/391 [00:14<00:02, 24.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 342/391 [00:14<00:02, 23.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 345/391 [00:14<00:01, 23.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▉ | 348/391 [00:14<00:01, 24.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|████████▉ | 351/391 [00:14<00:01, 24.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 354/391 [00:15<00:01, 23.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████▏| 357/391 [00:15<00:01, 24.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 360/391 [00:15<00:01, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 363/391 [00:15<00:01, 24.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▎| 366/391 [00:15<00:01, 23.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 369/391 [00:15<00:00, 24.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▌| 372/391 [00:15<00:00, 24.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 375/391 [00:15<00:00, 24.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 378/391 [00:16<00:00, 24.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 381/391 [00:16<00:00, 24.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 384/391 [00:16<00:00, 25.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▉| 387/391 [00:16<00:00, 25.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 391/391 [00:16<00:00, 23.56batch/s]\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/196 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3%|▎         | 5/196 [00:00<00:04, 43.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   5%|▌         | 10/196 [00:00<00:03, 46.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   8%|▊         | 16/196 [00:00<00:03, 50.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  11%|█         | 22/196 [00:00<00:03, 51.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  14%|█▍        | 28/196 [00:00<00:03, 52.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  17%|█▋        | 34/196 [00:00<00:03, 52.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  20%|██        | 40/196 [00:00<00:02, 53.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  23%|██▎       | 46/196 [00:00<00:02, 50.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  27%|██▋       | 52/196 [00:01<00:02, 49.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  30%|██▉       | 58/196 [00:01<00:02, 50.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  33%|███▎      | 64/196 [00:01<00:02, 51.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  36%|███▌      | 70/196 [00:01<00:02, 52.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  39%|███▉      | 76/196 [00:01<00:02, 53.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  42%|████▏     | 82/196 [00:01<00:02, 52.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  45%|████▍     | 88/196 [00:01<00:02, 52.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48%|████▊     | 94/196 [00:01<00:01, 52.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  51%|█████     | 100/196 [00:01<00:01, 49.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 105/196 [00:02<00:01, 48.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  57%|█████▋    | 111/196 [00:02<00:01, 49.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 116/196 [00:02<00:01, 49.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 122/196 [00:02<00:01, 49.49batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  65%|██████▍   | 127/196 [00:02<00:01, 49.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 132/196 [00:02<00:01, 49.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  70%|███████   | 138/196 [00:02<00:01, 49.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 144/196 [00:02<00:01, 50.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 150/196 [00:02<00:00, 46.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 155/196 [00:03<00:00, 46.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 160/196 [00:03<00:00, 46.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 166/196 [00:03<00:00, 48.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 171/196 [00:03<00:00, 47.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|████████▉ | 176/196 [00:03<00:00, 47.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 181/196 [00:03<00:00, 47.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▍| 186/196 [00:03<00:00, 48.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 191/196 [00:03<00:00, 48.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 196/196 [00:03<00:00, 49.37batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10\n",
            "Train Loss: 0.11 | Train Accuracy: 96.17%\n",
            "Validation Loss: 0.45 | Validation Accuracy: 85.44%\n",
            "Epoch Time: 20.60 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/391 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 3/391 [00:00<00:17, 22.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 6/391 [00:00<00:16, 23.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 9/391 [00:00<00:15, 24.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 12/391 [00:00<00:15, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 15/391 [00:00<00:15, 24.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▍         | 18/391 [00:00<00:16, 23.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 21/391 [00:00<00:16, 22.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 24/391 [00:01<00:17, 20.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 27/391 [00:01<00:17, 21.12batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 30/391 [00:01<00:16, 21.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 33/391 [00:01<00:16, 22.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 36/391 [00:01<00:16, 21.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 39/391 [00:01<00:16, 21.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 42/391 [00:01<00:16, 21.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 45/391 [00:02<00:16, 20.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 48/391 [00:02<00:16, 21.23batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 51/391 [00:02<00:15, 21.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 54/391 [00:02<00:15, 22.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▍        | 57/391 [00:02<00:14, 22.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 60/391 [00:02<00:14, 22.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 63/391 [00:02<00:14, 22.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 66/391 [00:03<00:16, 19.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 69/391 [00:03<00:16, 19.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 72/391 [00:03<00:15, 19.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 75/391 [00:03<00:16, 19.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 78/391 [00:03<00:15, 20.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 81/391 [00:03<00:15, 20.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██▏       | 84/391 [00:03<00:15, 19.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 86/391 [00:04<00:16, 18.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 88/391 [00:04<00:16, 18.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 90/391 [00:04<00:16, 18.23batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▎       | 92/391 [00:04<00:16, 18.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 95/391 [00:04<00:14, 20.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▌       | 98/391 [00:04<00:13, 21.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 101/391 [00:04<00:12, 22.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 104/391 [00:04<00:12, 23.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 107/391 [00:05<00:12, 23.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 110/391 [00:05<00:11, 23.78batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 113/391 [00:05<00:11, 23.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|██▉       | 116/391 [00:05<00:11, 24.16batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|███       | 119/391 [00:05<00:11, 24.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 122/391 [00:05<00:10, 24.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 125/391 [00:05<00:10, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 128/391 [00:05<00:10, 24.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▎      | 131/391 [00:05<00:10, 24.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 134/391 [00:06<00:10, 24.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▌      | 137/391 [00:06<00:10, 24.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 140/391 [00:06<00:10, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 143/391 [00:06<00:10, 24.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 146/391 [00:06<00:09, 24.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 149/391 [00:06<00:09, 24.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 152/391 [00:06<00:09, 24.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|███▉      | 155/391 [00:06<00:09, 23.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|████      | 158/391 [00:07<00:09, 23.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 161/391 [00:07<00:09, 24.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 164/391 [00:07<00:09, 24.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 167/391 [00:07<00:09, 24.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 170/391 [00:07<00:09, 24.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▍     | 173/391 [00:07<00:08, 24.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▌     | 176/391 [00:07<00:08, 24.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 179/391 [00:07<00:08, 24.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 182/391 [00:08<00:08, 24.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 185/391 [00:08<00:08, 24.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 188/391 [00:08<00:08, 24.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 191/391 [00:08<00:08, 24.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|████▉     | 194/391 [00:08<00:08, 24.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|█████     | 197/391 [00:08<00:07, 24.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 200/391 [00:08<00:07, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 203/391 [00:08<00:07, 24.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 206/391 [00:09<00:07, 24.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 209/391 [00:09<00:07, 24.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 212/391 [00:09<00:07, 24.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▍    | 215/391 [00:09<00:07, 24.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 218/391 [00:09<00:07, 24.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 221/391 [00:09<00:06, 24.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 224/391 [00:09<00:06, 24.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 227/391 [00:09<00:06, 24.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 230/391 [00:10<00:06, 23.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|█████▉    | 233/391 [00:10<00:06, 23.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|██████    | 236/391 [00:10<00:06, 24.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 239/391 [00:10<00:06, 24.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 242/391 [00:10<00:06, 24.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 245/391 [00:10<00:05, 24.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 248/391 [00:10<00:05, 24.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 251/391 [00:10<00:05, 24.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▍   | 254/391 [00:11<00:05, 24.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 257/391 [00:11<00:05, 24.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▋   | 260/391 [00:11<00:05, 24.49batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 263/391 [00:11<00:05, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 266/391 [00:11<00:05, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 269/391 [00:11<00:04, 24.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|██████▉   | 272/391 [00:11<00:04, 24.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|███████   | 275/391 [00:11<00:04, 24.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 278/391 [00:12<00:04, 24.23batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 281/391 [00:12<00:04, 23.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 284/391 [00:12<00:04, 24.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 287/391 [00:12<00:04, 24.23batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 290/391 [00:12<00:04, 24.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▍  | 293/391 [00:12<00:03, 24.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 296/391 [00:12<00:03, 24.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▋  | 299/391 [00:12<00:03, 24.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 302/391 [00:12<00:03, 24.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 305/391 [00:13<00:03, 24.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 308/391 [00:13<00:03, 24.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|███████▉  | 311/391 [00:13<00:03, 24.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|████████  | 314/391 [00:13<00:03, 24.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 317/391 [00:13<00:03, 24.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 320/391 [00:13<00:02, 24.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 323/391 [00:13<00:02, 24.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 326/391 [00:13<00:02, 24.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 329/391 [00:14<00:02, 24.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▍ | 332/391 [00:14<00:02, 24.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 335/391 [00:14<00:02, 24.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▋ | 338/391 [00:14<00:02, 23.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 341/391 [00:14<00:02, 22.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 344/391 [00:14<00:02, 22.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▊ | 347/391 [00:14<00:01, 22.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|████████▉ | 350/391 [00:15<00:01, 20.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|█████████ | 353/391 [00:15<00:01, 21.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 356/391 [00:15<00:01, 21.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 359/391 [00:15<00:01, 20.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 362/391 [00:15<00:01, 20.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 365/391 [00:15<00:01, 20.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 368/391 [00:15<00:01, 21.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▍| 371/391 [00:16<00:00, 21.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 374/391 [00:16<00:00, 21.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▋| 377/391 [00:16<00:00, 21.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 380/391 [00:16<00:00, 21.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 383/391 [00:16<00:00, 21.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▊| 386/391 [00:16<00:00, 20.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 391/391 [00:17<00:00, 22.97batch/s]\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/196 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   2%|▏         | 4/196 [00:00<00:05, 36.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   4%|▍         | 8/196 [00:00<00:05, 34.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   6%|▌         | 12/196 [00:00<00:05, 35.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   8%|▊         | 16/196 [00:00<00:05, 34.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10%|█         | 20/196 [00:00<00:05, 33.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▏        | 24/196 [00:00<00:05, 34.25batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  14%|█▍        | 28/196 [00:00<00:04, 35.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  16%|█▋        | 32/196 [00:00<00:04, 34.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18%|█▊        | 36/196 [00:01<00:04, 33.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  20%|██        | 40/196 [00:01<00:04, 33.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  22%|██▏       | 44/196 [00:01<00:04, 32.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  24%|██▍       | 48/196 [00:01<00:04, 32.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  27%|██▋       | 53/196 [00:01<00:03, 36.23batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  30%|██▉       | 58/196 [00:01<00:03, 39.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  32%|███▏      | 63/196 [00:01<00:03, 42.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  35%|███▍      | 68/196 [00:01<00:02, 44.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37%|███▋      | 73/196 [00:01<00:02, 42.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40%|███▉      | 78/196 [00:02<00:02, 44.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  42%|████▏     | 83/196 [00:02<00:02, 45.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  45%|████▌     | 89/196 [00:02<00:02, 47.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48%|████▊     | 94/196 [00:02<00:02, 48.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  51%|█████     | 99/196 [00:02<00:01, 48.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  53%|█████▎    | 104/196 [00:02<00:01, 48.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 109/196 [00:02<00:01, 48.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  58%|█████▊    | 114/196 [00:02<00:01, 48.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 119/196 [00:02<00:01, 47.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  63%|██████▎   | 124/196 [00:03<00:01, 45.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▌   | 129/196 [00:03<00:01, 46.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  68%|██████▊   | 134/196 [00:03<00:01, 47.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████   | 139/196 [00:03<00:01, 47.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  73%|███████▎  | 144/196 [00:03<00:01, 47.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  76%|███████▌  | 149/196 [00:03<00:00, 48.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▊  | 154/196 [00:03<00:00, 46.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  81%|████████  | 159/196 [00:03<00:00, 47.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▎ | 164/196 [00:03<00:00, 47.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  86%|████████▌ | 169/196 [00:03<00:00, 44.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 174/196 [00:04<00:00, 45.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  91%|█████████▏| 179/196 [00:04<00:00, 45.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 184/196 [00:04<00:00, 45.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  96%|█████████▋| 189/196 [00:04<00:00, 46.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 196/196 [00:04<00:00, 42.97batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10\n",
            "Train Loss: 0.11 | Train Accuracy: 96.25%\n",
            "Validation Loss: 0.46 | Validation Accuracy: 85.39%\n",
            "Epoch Time: 21.61 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/391 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 3/391 [00:00<00:17, 22.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 6/391 [00:00<00:16, 23.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 9/391 [00:00<00:16, 23.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 12/391 [00:00<00:16, 23.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 15/391 [00:00<00:15, 23.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▍         | 18/391 [00:00<00:15, 24.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 21/391 [00:00<00:15, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 24/391 [00:00<00:14, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 27/391 [00:01<00:14, 24.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 30/391 [00:01<00:14, 24.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 33/391 [00:01<00:14, 24.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 36/391 [00:01<00:14, 24.37batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 39/391 [00:01<00:14, 24.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 42/391 [00:01<00:14, 24.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 45/391 [00:01<00:13, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 48/391 [00:01<00:13, 25.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 51/391 [00:02<00:13, 25.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 54/391 [00:02<00:13, 24.59batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▍        | 57/391 [00:02<00:13, 24.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 60/391 [00:02<00:13, 24.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 63/391 [00:02<00:13, 24.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 66/391 [00:02<00:13, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 69/391 [00:02<00:12, 24.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 72/391 [00:02<00:13, 24.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 75/391 [00:03<00:13, 23.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 78/391 [00:03<00:13, 23.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 81/391 [00:03<00:12, 24.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██▏       | 84/391 [00:03<00:12, 23.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 87/391 [00:03<00:12, 24.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 90/391 [00:03<00:12, 24.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 93/391 [00:03<00:12, 24.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▍       | 96/391 [00:03<00:11, 24.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▌       | 99/391 [00:04<00:11, 24.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 102/391 [00:04<00:11, 24.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 105/391 [00:04<00:11, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 108/391 [00:04<00:11, 24.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 111/391 [00:04<00:11, 24.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 114/391 [00:04<00:11, 24.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|██▉       | 117/391 [00:04<00:11, 24.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 120/391 [00:04<00:10, 24.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███▏      | 123/391 [00:05<00:10, 24.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 126/391 [00:05<00:10, 24.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 129/391 [00:05<00:10, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 132/391 [00:05<00:10, 24.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▍      | 135/391 [00:05<00:10, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▌      | 138/391 [00:05<00:10, 24.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 141/391 [00:05<00:10, 24.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 144/391 [00:05<00:09, 24.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 147/391 [00:05<00:09, 24.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 150/391 [00:06<00:09, 24.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 153/391 [00:06<00:09, 24.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|███▉      | 156/391 [00:06<00:09, 24.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 159/391 [00:06<00:09, 24.04batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████▏     | 162/391 [00:06<00:09, 24.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 165/391 [00:06<00:09, 24.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 168/391 [00:06<00:09, 24.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▎     | 171/391 [00:06<00:09, 23.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▍     | 174/391 [00:07<00:09, 22.49batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▌     | 177/391 [00:07<00:09, 22.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 180/391 [00:07<00:10, 21.00batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 183/391 [00:07<00:10, 20.78batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 186/391 [00:07<00:09, 21.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 189/391 [00:07<00:09, 21.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 192/391 [00:07<00:09, 21.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|████▉     | 195/391 [00:08<00:09, 21.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 198/391 [00:08<00:09, 21.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████▏    | 201/391 [00:08<00:09, 20.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 204/391 [00:08<00:08, 21.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 207/391 [00:08<00:08, 21.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▎    | 210/391 [00:08<00:08, 21.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 213/391 [00:08<00:08, 21.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▌    | 216/391 [00:09<00:07, 22.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▌    | 219/391 [00:09<00:07, 21.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 222/391 [00:09<00:08, 20.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 225/391 [00:09<00:08, 20.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 228/391 [00:09<00:08, 20.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 231/391 [00:09<00:07, 20.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|█████▉    | 234/391 [00:10<00:07, 20.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 237/391 [00:10<00:07, 21.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████▏   | 240/391 [00:10<00:07, 21.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 243/391 [00:10<00:07, 19.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 245/391 [00:10<00:07, 18.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 247/391 [00:10<00:07, 18.67batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▎   | 249/391 [00:10<00:07, 18.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 252/391 [00:10<00:07, 19.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▌   | 255/391 [00:11<00:06, 21.42batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 258/391 [00:11<00:05, 22.45batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 261/391 [00:11<00:05, 23.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 264/391 [00:11<00:05, 23.18batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 267/391 [00:11<00:05, 23.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 270/391 [00:11<00:05, 23.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|██████▉   | 273/391 [00:11<00:04, 23.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 276/391 [00:11<00:04, 24.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████▏  | 279/391 [00:12<00:04, 24.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 282/391 [00:12<00:04, 24.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 285/391 [00:12<00:04, 24.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▎  | 288/391 [00:12<00:04, 23.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 291/391 [00:12<00:04, 24.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▌  | 294/391 [00:12<00:04, 23.98batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 297/391 [00:12<00:03, 24.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 300/391 [00:12<00:03, 24.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 303/391 [00:13<00:03, 24.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 306/391 [00:13<00:03, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 309/391 [00:13<00:03, 24.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|███████▉  | 312/391 [00:13<00:03, 24.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 315/391 [00:13<00:03, 24.19batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████▏ | 318/391 [00:13<00:03, 24.07batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 321/391 [00:13<00:02, 24.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 324/391 [00:13<00:02, 24.43batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▎ | 327/391 [00:14<00:02, 24.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 330/391 [00:14<00:02, 24.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▌ | 333/391 [00:14<00:02, 24.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 336/391 [00:14<00:02, 24.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 339/391 [00:14<00:02, 24.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 342/391 [00:14<00:01, 24.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 345/391 [00:14<00:01, 24.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▉ | 348/391 [00:14<00:01, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|████████▉ | 351/391 [00:14<00:01, 24.83batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 354/391 [00:15<00:01, 24.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████▏| 357/391 [00:15<00:01, 24.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 360/391 [00:15<00:01, 24.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 363/391 [00:15<00:01, 24.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▎| 366/391 [00:15<00:01, 24.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 369/391 [00:15<00:00, 24.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▌| 372/391 [00:15<00:00, 24.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 375/391 [00:15<00:00, 24.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 378/391 [00:16<00:00, 24.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 381/391 [00:16<00:00, 24.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 384/391 [00:16<00:00, 24.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▉| 387/391 [00:16<00:00, 23.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 391/391 [00:16<00:00, 23.54batch/s]\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/196 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   3%|▎         | 6/196 [00:00<00:03, 53.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   6%|▌         | 12/196 [00:00<00:03, 51.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   9%|▉         | 18/196 [00:00<00:03, 51.49batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▏        | 24/196 [00:00<00:03, 52.99batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  15%|█▌        | 30/196 [00:00<00:03, 52.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18%|█▊        | 36/196 [00:00<00:03, 52.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  21%|██▏       | 42/196 [00:00<00:03, 49.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  24%|██▍       | 48/196 [00:00<00:02, 50.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28%|██▊       | 54/196 [00:01<00:02, 51.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  31%|███       | 60/196 [00:01<00:02, 50.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  34%|███▎      | 66/196 [00:01<00:02, 51.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  37%|███▋      | 72/196 [00:01<00:02, 51.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40%|███▉      | 78/196 [00:01<00:02, 50.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43%|████▎     | 84/196 [00:01<00:02, 51.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  46%|████▌     | 90/196 [00:01<00:02, 48.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48%|████▊     | 95/196 [00:01<00:02, 48.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  51%|█████     | 100/196 [00:01<00:01, 48.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 105/196 [00:02<00:01, 48.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 110/196 [00:02<00:01, 46.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▉    | 116/196 [00:02<00:01, 48.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  62%|██████▏   | 121/196 [00:02<00:01, 48.31batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 126/196 [00:02<00:01, 48.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  67%|██████▋   | 131/196 [00:02<00:01, 48.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 136/196 [00:02<00:01, 46.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  72%|███████▏  | 141/196 [00:02<00:01, 45.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 146/196 [00:02<00:01, 46.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 151/196 [00:03<00:00, 47.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  80%|███████▉  | 156/196 [00:03<00:00, 46.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 161/196 [00:03<00:00, 47.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  85%|████████▍ | 166/196 [00:03<00:00, 47.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  88%|████████▊ | 172/196 [00:03<00:00, 48.78batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  90%|█████████ | 177/196 [00:03<00:00, 48.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  93%|█████████▎| 182/196 [00:03<00:00, 47.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  95%|█████████▌| 187/196 [00:03<00:00, 45.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 196/196 [00:04<00:00, 48.93batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10\n",
            "Train Loss: 0.10 | Train Accuracy: 96.54%\n",
            "Validation Loss: 0.47 | Validation Accuracy: 85.38%\n",
            "Epoch Time: 20.64 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training:   0%|          | 0/391 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   1%|          | 3/391 [00:00<00:17, 22.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 6/391 [00:00<00:18, 20.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   2%|▏         | 9/391 [00:00<00:18, 20.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   3%|▎         | 12/391 [00:00<00:18, 20.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   4%|▍         | 15/391 [00:00<00:18, 20.40batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▍         | 18/391 [00:00<00:18, 20.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   5%|▌         | 21/391 [00:01<00:17, 20.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   6%|▌         | 24/391 [00:01<00:18, 20.06batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   7%|▋         | 27/391 [00:01<00:18, 19.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 30/391 [00:01<00:17, 20.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   8%|▊         | 33/391 [00:01<00:17, 20.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:   9%|▉         | 36/391 [00:01<00:17, 20.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  10%|▉         | 39/391 [00:01<00:17, 20.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  11%|█         | 42/391 [00:02<00:16, 20.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 45/391 [00:02<00:16, 21.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  12%|█▏        | 48/391 [00:02<00:16, 21.26batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  13%|█▎        | 51/391 [00:02<00:16, 21.15batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  14%|█▍        | 54/391 [00:02<00:16, 20.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▍        | 57/391 [00:02<00:16, 20.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  15%|█▌        | 60/391 [00:02<00:16, 20.20batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  16%|█▌        | 63/391 [00:03<00:16, 19.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  17%|█▋        | 66/391 [00:03<00:15, 20.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 69/391 [00:03<00:16, 19.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  18%|█▊        | 72/391 [00:03<00:15, 20.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  19%|█▉        | 75/391 [00:03<00:15, 19.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|█▉        | 78/391 [00:03<00:15, 19.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  20%|██        | 80/391 [00:03<00:15, 19.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  21%|██        | 82/391 [00:04<00:16, 19.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  22%|██▏       | 85/391 [00:04<00:14, 20.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 88/391 [00:04<00:13, 22.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  23%|██▎       | 91/391 [00:04<00:13, 22.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  24%|██▍       | 94/391 [00:04<00:12, 23.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  25%|██▍       | 97/391 [00:04<00:12, 23.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▌       | 100/391 [00:04<00:12, 23.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  26%|██▋       | 103/391 [00:04<00:11, 24.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  27%|██▋       | 106/391 [00:05<00:11, 24.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  28%|██▊       | 109/391 [00:05<00:11, 24.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▊       | 112/391 [00:05<00:11, 24.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  29%|██▉       | 115/391 [00:05<00:11, 24.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  30%|███       | 118/391 [00:05<00:11, 24.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  31%|███       | 121/391 [00:05<00:10, 24.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 124/391 [00:05<00:10, 24.33batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  32%|███▏      | 127/391 [00:05<00:10, 24.61batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  33%|███▎      | 130/391 [00:05<00:10, 24.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  34%|███▍      | 133/391 [00:06<00:10, 24.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  35%|███▍      | 136/391 [00:06<00:10, 24.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▌      | 139/391 [00:06<00:10, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  36%|███▋      | 142/391 [00:06<00:10, 24.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  37%|███▋      | 145/391 [00:06<00:10, 24.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  38%|███▊      | 148/391 [00:06<00:10, 24.25batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▊      | 151/391 [00:06<00:09, 24.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  39%|███▉      | 154/391 [00:06<00:09, 24.52batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  40%|████      | 157/391 [00:07<00:09, 24.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  41%|████      | 160/391 [00:07<00:09, 24.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 163/391 [00:07<00:09, 24.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  42%|████▏     | 166/391 [00:07<00:09, 24.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  43%|████▎     | 169/391 [00:07<00:08, 24.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  44%|████▍     | 172/391 [00:07<00:08, 24.54batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  45%|████▍     | 175/391 [00:07<00:08, 24.41batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▌     | 178/391 [00:07<00:08, 24.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  46%|████▋     | 181/391 [00:08<00:08, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  47%|████▋     | 184/391 [00:08<00:08, 24.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  48%|████▊     | 187/391 [00:08<00:08, 24.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▊     | 190/391 [00:08<00:08, 24.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  49%|████▉     | 193/391 [00:08<00:08, 24.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  50%|█████     | 196/391 [00:08<00:07, 24.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  51%|█████     | 199/391 [00:08<00:07, 24.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 202/391 [00:08<00:07, 24.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  52%|█████▏    | 205/391 [00:09<00:07, 24.65batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  53%|█████▎    | 208/391 [00:09<00:07, 24.81batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  54%|█████▍    | 211/391 [00:09<00:07, 24.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▍    | 214/391 [00:09<00:07, 24.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  55%|█████▌    | 217/391 [00:09<00:06, 24.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  56%|█████▋    | 220/391 [00:09<00:07, 24.11batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  57%|█████▋    | 223/391 [00:09<00:07, 23.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  58%|█████▊    | 226/391 [00:09<00:06, 24.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▊    | 229/391 [00:10<00:06, 24.08batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  59%|█████▉    | 232/391 [00:10<00:06, 24.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  60%|██████    | 235/391 [00:10<00:06, 24.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  61%|██████    | 238/391 [00:10<00:06, 24.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 241/391 [00:10<00:06, 24.73batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  62%|██████▏   | 244/391 [00:10<00:06, 24.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  63%|██████▎   | 247/391 [00:10<00:06, 23.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  64%|██████▍   | 250/391 [00:10<00:05, 24.26batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▍   | 253/391 [00:11<00:05, 24.38batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  65%|██████▌   | 256/391 [00:11<00:05, 24.46batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  66%|██████▌   | 259/391 [00:11<00:05, 24.30batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  67%|██████▋   | 262/391 [00:11<00:05, 24.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  68%|██████▊   | 265/391 [00:11<00:05, 24.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▊   | 268/391 [00:11<00:04, 25.01batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  69%|██████▉   | 271/391 [00:11<00:04, 24.51batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  70%|███████   | 274/391 [00:11<00:04, 24.48batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  71%|███████   | 277/391 [00:11<00:04, 24.76batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 280/391 [00:12<00:04, 24.71batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  72%|███████▏  | 283/391 [00:12<00:04, 24.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  73%|███████▎  | 286/391 [00:12<00:04, 24.94batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  74%|███████▍  | 289/391 [00:12<00:04, 24.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▍  | 292/391 [00:12<00:04, 24.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  75%|███████▌  | 295/391 [00:12<00:04, 23.75batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  76%|███████▌  | 298/391 [00:12<00:03, 24.10batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  77%|███████▋  | 301/391 [00:12<00:03, 24.55batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  78%|███████▊  | 304/391 [00:13<00:03, 24.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▊  | 307/391 [00:13<00:03, 24.93batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  79%|███████▉  | 310/391 [00:13<00:03, 24.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  80%|████████  | 313/391 [00:13<00:03, 24.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  81%|████████  | 316/391 [00:13<00:03, 24.82batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 319/391 [00:13<00:02, 24.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  82%|████████▏ | 322/391 [00:13<00:02, 24.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  83%|████████▎ | 325/391 [00:13<00:02, 24.72batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  84%|████████▍ | 328/391 [00:14<00:02, 24.79batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▍ | 331/391 [00:14<00:02, 23.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  85%|████████▌ | 334/391 [00:14<00:02, 22.56batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  86%|████████▌ | 337/391 [00:14<00:02, 22.88batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  87%|████████▋ | 340/391 [00:14<00:02, 22.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 343/391 [00:14<00:02, 21.09batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  88%|████████▊ | 346/391 [00:14<00:02, 21.60batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  89%|████████▉ | 349/391 [00:15<00:02, 20.66batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  90%|█████████ | 352/391 [00:15<00:01, 20.62batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  91%|█████████ | 355/391 [00:15<00:01, 20.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 358/391 [00:15<00:01, 20.97batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  92%|█████████▏| 361/391 [00:15<00:01, 21.53batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  93%|█████████▎| 364/391 [00:15<00:01, 21.39batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  94%|█████████▍| 367/391 [00:15<00:01, 21.68batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▍| 370/391 [00:16<00:00, 22.12batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  95%|█████████▌| 373/391 [00:16<00:00, 22.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  96%|█████████▌| 376/391 [00:16<00:00, 22.28batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  97%|█████████▋| 379/391 [00:16<00:00, 21.86batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 382/391 [00:16<00:00, 20.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  98%|█████████▊| 385/391 [00:16<00:00, 20.27batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training:  99%|█████████▉| 388/391 [00:16<00:00, 19.49batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Training: 100%|██████████| 391/391 [00:17<00:00, 22.87batch/s]\n",
            "\n",
            "\n",
            "Evaluating:   0%|          | 0/196 [00:00<?, ?batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   2%|▏         | 4/196 [00:00<00:05, 37.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   4%|▍         | 8/196 [00:00<00:05, 37.12batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   6%|▌         | 12/196 [00:00<00:05, 35.57batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:   8%|▊         | 16/196 [00:00<00:04, 36.44batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  10%|█         | 20/196 [00:00<00:04, 36.32batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  12%|█▏        | 24/196 [00:00<00:04, 35.90batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  14%|█▍        | 28/196 [00:00<00:04, 35.63batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  16%|█▋        | 32/196 [00:00<00:04, 35.02batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  18%|█▊        | 36/196 [00:01<00:04, 35.89batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  20%|██        | 40/196 [00:01<00:04, 35.47batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  22%|██▏       | 44/196 [00:01<00:04, 34.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  24%|██▍       | 48/196 [00:01<00:04, 34.24batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  28%|██▊       | 54/196 [00:01<00:03, 39.64batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  30%|███       | 59/196 [00:01<00:03, 40.80batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  33%|███▎      | 64/196 [00:01<00:03, 41.91batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  35%|███▌      | 69/196 [00:01<00:02, 43.03batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  38%|███▊      | 74/196 [00:01<00:02, 43.22batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  40%|████      | 79/196 [00:02<00:02, 44.14batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  43%|████▎     | 84/196 [00:02<00:02, 45.74batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  45%|████▌     | 89/196 [00:02<00:02, 46.17batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  48%|████▊     | 94/196 [00:02<00:02, 47.13batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  51%|█████     | 100/196 [00:02<00:02, 47.26batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  54%|█████▎    | 105/196 [00:02<00:01, 46.70batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  56%|█████▌    | 110/196 [00:02<00:01, 44.34batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  59%|█████▊    | 115/196 [00:02<00:01, 45.35batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  61%|██████    | 120/196 [00:02<00:01, 43.36batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  64%|██████▍   | 125/196 [00:03<00:01, 44.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  66%|██████▋   | 130/196 [00:03<00:01, 45.84batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  69%|██████▉   | 135/196 [00:03<00:01, 45.77batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  71%|███████▏  | 140/196 [00:03<00:01, 45.95batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  74%|███████▍  | 145/196 [00:03<00:01, 46.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  77%|███████▋  | 150/196 [00:03<00:01, 45.29batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  79%|███████▉  | 155/196 [00:03<00:00, 43.92batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  82%|████████▏ | 160/196 [00:03<00:00, 44.85batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  84%|████████▍ | 165/196 [00:03<00:00, 44.50batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  87%|████████▋ | 170/196 [00:04<00:00, 44.58batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  89%|████████▉ | 175/196 [00:04<00:00, 44.69batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  92%|█████████▏| 180/196 [00:04<00:00, 45.05batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  94%|█████████▍| 185/196 [00:04<00:00, 45.87batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating:  97%|█████████▋| 190/196 [00:04<00:00, 46.21batch/s]\u001b[A\u001b[A\n",
            "\n",
            "Evaluating: 100%|██████████| 196/196 [00:04<00:00, 42.56batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10\n",
            "Train Loss: 0.10 | Train Accuracy: 96.67%\n",
            "Validation Loss: 0.47 | Validation Accuracy: 85.34%\n",
            "Epoch Time: 21.72 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# adding a classification head to the BERT model\n",
        "class BertWithClassificationHead(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super(BertWithClassificationHead, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(bert_model.config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        outputs = self.bert(input_ids)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "# instantiate the model with the classification head\n",
        "model_with_classifier = BertWithClassificationHead(bert_model, num_classes)\n",
        "\n",
        "model_with_classifier.to(device)\n",
        "\n",
        "optimizer = AdamW(model_with_classifier.parameters(), lr=2e-5)\n",
        "\n",
        "# total number of training steps is [number of batches] x [number of epochs]\n",
        "total_steps = len(train_loader) * epochs\n",
        "\n",
        "#  learning rate scheduler:\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,  # default value in transformers\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "def train_model(model, dataloader, optimizer, loss_function, scheduler):\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    model.train()\n",
        "\n",
        "    progress_bar = tqdm(total=len(dataloader), desc=\"Training\", unit=\"batch\")\n",
        "\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        logits = model(inputs)\n",
        "        loss = loss_function(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        accuracy = compute_accuracy(logits.cpu(), labels.cpu())\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy += accuracy\n",
        "\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    average_accuracy = total_accuracy / len(dataloader)\n",
        "\n",
        "    return average_loss, average_accuracy\n",
        "\n",
        "lowest_val_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    training_loss, training_accuracy = train_model(model_with_classifier, train_loader, optimizer, loss_function, scheduler)\n",
        "\n",
        "    validation_loss, validation_accuracy = evaluate_model(model_with_classifier, validation_loader, loss_function)\n",
        "\n",
        "    # save the model if the validation loss is the best we've seen so far\n",
        "    if validation_loss < lowest_val_loss:\n",
        "        torch.save(model_with_classifier.state_dict(), '/content/drive/My Drive/nlp_hw4/best_model.pt')\n",
        "        lowest_val_loss = validation_loss\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}')\n",
        "    print(f'Train Loss: {training_loss:.2f} | Train Accuracy: {training_accuracy * 100:.2f}%')\n",
        "    print(f'Validation Loss: {validation_loss:.2f} | Validation Accuracy: {validation_accuracy * 100:.2f}%')\n",
        "    print(f'Epoch Time: {time.time() - start_time:.2f} seconds')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vodVOwqpf9LX",
        "outputId": "26827ac6-80e6-41a5-d19d-ae94c2f19c5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to /content/drive/My Drive/nlp_hw4/y_test3.txt\n"
          ]
        }
      ],
      "source": [
        "# function to preprocess and encode test data\n",
        "def preprocess_and_encode_test_data(test_df, tokenizer, max_seq_len=128):\n",
        "    test_df_clean = preprocess_text_data_fast(test_df, 'sentence')\n",
        "\n",
        "    test_encoded = encode_text_column(test_df_clean, 'sentence_clean', tokenizer)\n",
        "\n",
        "    # ensure the sequences are of maximum length with padding\n",
        "    class TextDataSequenceTest(Dataset):\n",
        "        def __init__(self, dataframe, max_seq_len=150):\n",
        "            self.dataframe = dataframe\n",
        "            self.max_seq_len = max_seq_len\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.dataframe)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            token_ids = self.dataframe.iloc[index]['sentence_clean_ids']\n",
        "            if len(token_ids) > self.max_seq_len:\n",
        "                token_ids = token_ids[:self.max_seq_len]\n",
        "            else:\n",
        "                token_ids += [0] * (self.max_seq_len - len(token_ids))\n",
        "\n",
        "            return {\n",
        "                'input_ids': torch.tensor(token_ids, dtype=torch.long)\n",
        "            }\n",
        "\n",
        "    test_dataset = TextDataSequenceTest(test_encoded, max_seq_len=max_seq_len)\n",
        "\n",
        "    return test_dataset\n",
        "\n",
        "test_dataset = preprocess_and_encode_test_data(df_test, bpe_tokenizer)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def predict_test(model, dataloader):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            inputs = batch['input_ids'].to(device)\n",
        "            outputs = model(inputs)\n",
        "            logits = outputs\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "    return all_predictions\n",
        "\n",
        "y_test_predictions = predict_test(model_with_classifier, test_loader)\n",
        "\n",
        "# save predictions to y_test3.txt\n",
        "output_file_path = '/content/drive/My Drive/nlp_hw4/y_test3.txt'\n",
        "with open(output_file_path, 'w') as file:\n",
        "    for prediction in y_test_predictions:\n",
        "        file.write(f\"{prediction:.1f}\\n\")\n",
        "\n",
        "print(f\"Predictions saved to {output_file_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0325e4ce4a9942469cf75e087980e669": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0351806b30c143e098b4ad22aaf8a80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d646c957bc4c4ec1a9b4c891895f67c8",
            "placeholder": "​",
            "style": "IPY_MODEL_b2fe59760f2d4c72b47c73a012b3e602",
            "value": "Map: 100%"
          }
        },
        "097d46637ea54c9b89467a89f9003f52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11437c7066284bdcaec01a5a4b4da35c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ffcfc6ff0cb4a8caaa39390fa1ef108": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "584b7f69251a469b9aa564e2ce25520c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ab4213a3357430088738141634ad504": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0351806b30c143e098b4ad22aaf8a80a",
              "IPY_MODEL_748e4c7b07f44917ae23de759345ebd1",
              "IPY_MODEL_d5c3eeb2cb9c426f9688943a3a7504e5"
            ],
            "layout": "IPY_MODEL_c9e11ec23fe049b58a35cf194126d0da"
          }
        },
        "64d2aa10b75442749def81be47867277": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd5b4ab2b79d461486b99c58f85f5e6d",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9bb168712c1846438dc29e8c6b85dbf1",
            "value": 25000
          }
        },
        "748e4c7b07f44917ae23de759345ebd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0325e4ce4a9942469cf75e087980e669",
            "max": 12500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_097d46637ea54c9b89467a89f9003f52",
            "value": 12500
          }
        },
        "7d37b3bf65404467afaaeabdf28e0a8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81bec07a5fac492b92e1b30d986bd0fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_855c69c0477d44108aa96cb4b7a78b22",
              "IPY_MODEL_64d2aa10b75442749def81be47867277",
              "IPY_MODEL_8edd59f8bc7a4789980de5c312d447cc"
            ],
            "layout": "IPY_MODEL_cb53914a9e1d40c29b6aab2c6e68fb2e"
          }
        },
        "855c69c0477d44108aa96cb4b7a78b22": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d145cc2396e247ca918f284ae011e0fe",
            "placeholder": "​",
            "style": "IPY_MODEL_11437c7066284bdcaec01a5a4b4da35c",
            "value": "Map: 100%"
          }
        },
        "8edd59f8bc7a4789980de5c312d447cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf89d3402ea445fca94344c562c4cb68",
            "placeholder": "​",
            "style": "IPY_MODEL_7d37b3bf65404467afaaeabdf28e0a8a",
            "value": " 25000/25000 [00:09&lt;00:00, 3219.97 examples/s]"
          }
        },
        "9bb168712c1846438dc29e8c6b85dbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2fe59760f2d4c72b47c73a012b3e602": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9e11ec23fe049b58a35cf194126d0da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb53914a9e1d40c29b6aab2c6e68fb2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf89d3402ea445fca94344c562c4cb68": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d145cc2396e247ca918f284ae011e0fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5c3eeb2cb9c426f9688943a3a7504e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_584b7f69251a469b9aa564e2ce25520c",
            "placeholder": "​",
            "style": "IPY_MODEL_2ffcfc6ff0cb4a8caaa39390fa1ef108",
            "value": " 12500/12500 [00:03&lt;00:00, 3237.90 examples/s]"
          }
        },
        "d646c957bc4c4ec1a9b4c891895f67c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd5b4ab2b79d461486b99c58f85f5e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
